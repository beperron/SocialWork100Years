---
title: 'Examining a Quarter Century of Publishing Trends in Social Work Research:
  A Data Science Perspective'
author: ''
output: pdf_document
---

`r knitr::opts_chunk$set(echo=FALSE, message=FALSE, warning=FALSE, comment=NA)`

```{r BEPinitialization, eval=TRUE}
rm(list=ls())  

load("/Users/beperron/Git/SocialWorkResearch/socialWorkHistory.Rdata")

# Load required packages
library(ggplot2)
library(dplyr)
library(gridExtra)
```

```{r BGVinitialization, eval=FALSE}
rm(list=ls())  
load("/Users/SSW/Documents/GitHub/SocialWork100Years/socialWorkHistory.Rdata")

# Load required packages
library(ggplot2)
library(dplyr)
library(gridExtra)
```

```{r imageMaker, comment=NA, echo=FALSE, eval=FALSE}
# Read data files
# load("/Users/beperron/Git/SocialWorkResearch/Data/ebscoFULL.R")
# load("/Users/beperron/Git/SocialWorkResearch/Data/proQuest.R")
# hodge <- readLines("/Users/beperron/Git/SocialWorkResearch/search_specifications/hodge.txt", n = -1)
# 
# source("/Users/beperron/Git/SocialWorkResearch/functions/combineData.R")
# source("/Users/beperron/Git/SocialWorkResearch/functions/cleaningJournals.R")
# source("/Users/beperron/Git/SocialWorkResearch/functions/shortArticles.R")
# source("/Users/beperron/Git/SocialWorkResearch/functions/titleMatching.R")
# source("/Users/beperron/Git/SocialWorkResearch/functions/otherDocuments.R")
# source("/Users/beperron/Git/SocialWorkResearch/functions/fewArticles.R")
# source("/Users/beperron/Git/SocialWorkResearch/functions/pretty.R")
# 
# combineData.f()
# cleaningJournals.f()
# shortArticles.f()
# titleMatching.f()
# otherDocuments.f()
# fewArticles.f()
#save.image(file = "socialWorkHistory.Rdata")
```

```{r methodSummaryData, eval=TRUE}
#append.articles <- length(which(full.append.df$attributes == "article"))
#append.journals <- filter(full.append.df, attributes == "journal") %>% 
#    summarise(Unique = n_distinct(record))

unique.titles <- filter(full.df, attributes == "journal") %>%
    group_by(record) %>%
    summarise(
        N = n())

articles.titleMatching <- length(which(full.df$attributes == "article"))

journals.titleMatching <- filter(full.df, attributes == "journal") %>%
        summarise(Unique = n_distinct(record))


articles.otherDocuments <- length(which(full.df$attributes == "article"))
journals.otherDocuments <- filter(full.df, attributes == "journal") %>% 
    summarise(Unique = n_distinct(record))


articles.final <- length(which(full.df$attributes == "article"))
journals.final <- filter(full.df, attributes == "journal") %>% 
    summarise(Unique = n_distinct(record))

```

```{r Table_1, comment=NA, echo=FALSE, eval=TRUE}
unique.titles <- filter(full.df, attributes == "journal") %>%
    group_by(record) %>%
    summarise(N = n())

unique.titles <- filter(full.df, attributes == "journal") %>%
    group_by(record) %>%
    mutate(Hodge.list = ifelse(record %in% hodge == TRUE, 1, 0)) %>%
    summarise(N = n(), 
              Hodge.list = max(Hodge.list)) %>%
    mutate(Hodge = ifelse(Hodge.list == 1, "Y", "N"))


n.so.yr <- filter(full.df, attributes == "journal" | attributes == "pubYear")

n.so <- filter(full.df, attributes == "journal") %>% mutate(title = record) %>% 
        select(-attributes, -record)

n.yr <- filter(full.df, attributes == "pubYear") %>% mutate(year = record ) %>% 
        select(-attributes, -record)

n.so.yr <- left_join(n.so, n.yr) %>%
    group_by(title) %>%
    summarise(first = min(year), last = max(year), n = n()) %>%
    arrange(title)

H <- unique.titles$Hodge

final.list <- cbind(n.so.yr, H)
final.list <- arrange(final.list, desc(n))
```

```{r plotArticleCount, eval=TRUE}
n.articles.year <- filter(full.df, attributes == "pubYear") 
year.split <- split(n.articles.year, n.articles.year$record)
year.count <- unlist(lapply(year.split, nrow))
year.count <- year.count[order(names(year.count))]
years <- names(year.count)

articles.year.df <- data.frame(years, year.count)
rownames(articles.year.df) <- NULL

plot.article.count <- ggplot(articles.year.df, aes(as.factor(years), 
                    y = year.count, group=1)) + 
    geom_line(colour="black") +
    #geom_point(colour="red") + 
    theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 8), title = element_text(size = 10), plot.title=element_text(size=10)) + 
    xlab("Year") + 
    ylab("Articles") + 
    ggtitle("A. Number of Articles by Year") +
    scale_x_discrete(breaks=c(seq(1989, 2013, 2))) +
    scale_y_continuous(breaks = c(seq(0, 2500, 200))) +
    annotate("rect", xmin = 2008-1989+1, xmax =  2012-1989+1, 
             ymin = 1700, ymax = 2050, alpha = .20, fill = "#0072B2") +
      annotate("rect", xmin = 2005-1989+1, xmax =  2007-1989+1, 
             ymin = 1400, ymax = 1750, alpha = .20, fill= "#0072B2")  
```

```{r plotJournalCount, eval=TRUE}
n.journals.year <- filter(full.df, attributes == "journal") %>%
    select(articleID, record)

n.year <- filter(full.df, attributes == "pubYear") %>%
    select(articleID, record)

n.journals.year <- left_join(n.journals.year, n.year, by = "articleID", copy=TRUE) %>%
    group_by(record.y) %>%
    summarise(Journal.count = n_distinct(record.x))

plot.journal.count <- ggplot(n.journals.year, aes(as.factor(record.y), 
                    y = Journal.count, group=1)) + 
    geom_line(colour="black") +
    #geom_point(colour="red") + 
    theme(axis.text.x = element_text(angle = 45, hjust = 1, size=8), title = element_text(size = 10), plot.title=element_text(size=10)) + 
    xlab("Year") + 
    ylab("Journals") + 
    ggtitle("B. Number of Journals by Year") +
    scale_x_discrete(breaks=c(seq(1989, 2013, 2))) +
    scale_y_continuous(breaks = c(seq(0, 100, 10)), limits = c(20, 70)) +
     annotate("rect", xmin = 2008-1989+1, xmax =  2012-1989+1, 
             ymin = 55, ymax = 70, alpha = .20, fill= "#0072B2") +
         annotate("rect", xmin = 2005-1989+1, xmax =  2007-1989+1, 
             ymin = 53, ymax = 62, alpha = .20, fill = "#0072B2")  
```

```{r plotAuthorCount, eval=TRUE}

year.df <- full.df %>%
        filter(attributes == "pubYear") %>%
        select(id = articleID, pubYear = record)

authors.df <- full.df %>%
        filter(attributes == "author") %>%
        select(id = articleID, author = record)


n_authors <- authors.df %>%
        group_by(id) %>%
        summarise(n=n())

n_authors <- n_authors%>% 
        left_join(year.df) %>%
        group_by(pubYear) %>%
        mutate(n = as.numeric(n)) %>%
        summarise(median.n = median(n),
                  average.n = mean(n),
                  min.n = min(n),
                  max.n = max(n),
                  std.dev  = sd(n) )

n_authors2 <- n_authors%>% 
       select(median.n, average.n, std.dev) %>%
       rename(Median = median.n, Average = average.n, Standard_Deviation =std.dev)


n_authors2$ID <- as.factor(c(1989:2013))

n_authors2_melted <- reshape2::melt(n_authors2, id.vars = "ID")
colnames(n_authors2_melted) <- c("pubYear", "summaryStat", "value")

cbPalette <- c("#999999", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")

plot.author.count <- ggplot(n_authors2_melted, aes(x = pubYear, y = value, group = summaryStat, colour = summaryStat)) + 
     geom_line(size = .75) + 
    theme(axis.text.x = element_text(angle = 45, hjust = 1), 
          title = element_text(size = 10), 
          legend.title=element_blank()) +
    scale_colour_manual(values=cbPalette)  +
    scale_x_discrete(breaks=c(seq(1989, 2013, 2))) +
    scale_y_continuous(limits = c(0,2.5)) +
    ylab("Value") +
    xlab("Year") +
    ggtitle("Authors Per Article by Year")

```

```{r nsf, eval=TRUE}
nsf <- data.frame(c(rep(NA, 13), 238, 273, 304, 323, 308, 287, 338, 332, 308, 289, 280, NA), as.factor(seq(1989,2013, 1)))

growth <- c(rep("Unknown", 14), rep("Increase", 2), rep("Decrease", 3), "Increase", 
     rep("Decrease", 4), "Unknown")
nsf <- cbind(nsf, growth)

colnames(nsf) <- c("n.grads", "years", "growth")

#write.csv(nsf, file="nsf.csv")

```

```{r gade, eval=TRUE}
years <- c(1989:2015)
programs <- rep(NA, 27)
gade <- as.data.frame(cbind(as.character(years), programs))
gade$programs <- as.numeric(gade$programs)
gade[15, 2] <- 72
gade[27, 2] <- 86
colnames(gade) <- c("years", "programs")
```

```{r pages, eval=TRUE}
pages.tmp <- full.df %>% filter(attributes == "pages")
years.tmp <- full.df %>% filter(attributes == "pubYear")
pages.df <- left_join(pages.tmp, years.tmp, by = "articleID") %>% 
    select(record.y, record.x) %>%
    rename(years = record.y, pages = record.x) %>%
    mutate(pages = as.numeric(pages)) %>%
    group_by(years) %>%
    summarise(
        Median.Pages = median(pages, na.rm=TRUE), 
        Mean.Pages = mean(pages, na.rm=TRUE),
        Std.Pages = sd(pages, na.rm=TRUE))

```

```{r nsfPlot, eval=TRUE}
plot.PhD <- ggplot(nsf, aes(years, n.grads, group=1)) + 
    geom_line(linetype="dotted", colour = "#0072B2", size = .75) + 
    geom_point(colour = "#0072B2", size = .75) +
     scale_y_continuous(breaks = c(seq(0, 350, 25)), limits = c(200,350)) +
     ylab("Doctoral degrees") +
     xlab("Year") +
     geom_text(aes(label = n.grads), vjust = -1.2, size = 2.75) + 
     theme(axis.text.x = element_text(angle = 45, hjust = 1), 
           title = element_text(size = 10)) +
     ggtitle("Number of Earned Social Work Doctoral Degrees by Year") 
```

```{r eval=TRUE}
colnames(n.journals.year) <- c("years", "journals")
n_authors2 <- rename(n_authors2, years = ID, Avg.Authors = Average, Mdn.Authors = Median, Sd.Authors = Standard_Deviation)


scholarly.df <- left_join(articles.year.df, n.journals.year, by = "years") 
scholarly.df <- left_join(scholarly.df, n_authors2, by = "years")
scholarly.df <- left_join(scholarly.df, nsf, by = "years")
scholarly.df <- left_join(scholarly.df, pages.df, by = "years")
scholarly.df[c(26, 27), ] <- rep(NA)
scholarly.df[26, 1] <- 2014
scholarly.df[27, 1] <- 2015

scholarly.df <- mutate(scholarly.df, years = as.numeric(years)) %>%
    rename(article.count = year.count, journal.count = year.count, program.growth = growth) 
scholarly.df$gade <- rep(NA, nrow(scholarly.df))  
scholarly.df$gade[15] <- 72
scholarly.df$gade[27] <- 86
```

```{r appendix_A_intersection}
unique.swHistory <- filter(full.df, attributes == "journal") %>%
                distinct(record)

hodge.swHistory.intersect <- intersect(hodge, unique.swHistory$record)

hodge.swHistory.diff <- setdiff(hodge, unique.swHistory$record)

```

### Data Revolution

It is estimated that 90% of all the data in human history has been generated in the past two years, and the amount of data continues to grow by roughly 2.5 quintillion bytes every day [(IBM, nd-a)](http://www-01.ibm.com/software/data/bigdata/what-is-big-data.html).  We have unprecedented opportunities for deriving actionable insights from these data to advance the human condition.  The significance continues to gain the attention of national and international organizations.  For example, the Independent Expert Advisory Group (IEAG) of the United Nations prepared a report entitled, _A World That Counts:  Mobilising the Data Revolution for Sustainable Development_ [(IEAG, 2014)](http://www.undatarevolution.org/report/).  The IEAG (2014) argues that "Data are the lifeblood of decision-making and the raw material for accountability.  Without high-quality data providing the right information on the right things at the right time; designing, monitoring and evaluating effective policies becomes almost impossible" (p. 2).  The United States federal government established an open data initiative, motivated by the awareness that data can have the greatest impact when it is "accessible, discoverable, and usable" [(www.data.gov)](www.data.gov).  Data from all levels of government that is made completely open can lead to "cost savings, efficiency, fuel for business, improved civic services, informed policy, performance planning, research and scientific discoveries, transparency and accountability, and increased public participation in the democratic dialogue" (www.data.gov).  At the time of preparing this article, over 124,000 data sets were freely available through www.data.gov among a diverse set of topic areas:  agriculture, business, climate, consumers, ecosystems, education, energy, finance, health, local government, manufacturing, ocean studies, public safety, and science and research.  

In 2012, the NIH established the Big Data to Knowledge (BD2K) initiative, recognizing the potential of data to advance our understanding of human health and disease.  This initiative was designed to support four broad goals:  1) to facilitate use of biomedical digital assets by making them discoverable, accessible, and citable; 2) to conduct research and develop methods, software and tools to analyze biomedical big data; 3) to enhance training in the development of use and tools necessary for biomedical big data science; and 4) to support a data ecosystem that accelerates discovery as part of the digital enterprise (NIH).  At roughly the same time, the National Science Foundation (NSF) created new initiatives to harness the potential of the data revolution, arguing that "data are motivating a profound transformation in the culture and conduct of scientific research in every field of science and engineering ... American scientists must rise to the challenges and seize the opportunities afforded by this new, data-driven revolution" (Suresh, XXX).  The programs of the NSF are organized around capacity building and innovative applications.  Capacity building activities focus on developing fundamental theories, techniques, methodologies and technologies that are broadly applicable to big data problems.  Innovative applications relate to specific domains advancing innovative ideas that provide solutions “with potential for a broader impact on data science and its applications.” 

### Data Science and Social Work

We are indeed living in a data revolution.  The relevance to social work research is clear — we have the opportunity to study human behavior and other social phenomena in real time, with vastly improved reliability in our measurements, and at scale.  Data comes from a wide range of sources, including (but not limited to) social media interactions, electronic medical records, audio and video recordings, click stream data, transaction data, web logs, embedded sensors, and GPS.  New tools allow blending of data sets, unique ways to analyze data, and methods of visualizations that can produce insights limited only by one's imagination.  As stated by the NIH, "lack of appropriate tools, poor data accessibility, and insufficient training, are major impediments to rapid translational impact" in this new era of data.  It is essential that the field of social work fully embraces the data revolution and considers the extent to which the existing research infrastructure and training is preparing both social work researchers and providers to be effective and efficient in a data rich world.  The potential of the data revolution is reflected in the current _Grand Challenge_ entitled, “Harness technology for social good.”  However, we believe that the data revolution goes beyond this single _Grand Challenge_ as it provides the opportunity to be informative in all areas of social work research.  

Data science serves as a useful framework for broadening the existing research infrastructure and training opportunities within social work.  Data science is similar to traditional academic research, as it includes the key roles of substantive expertise and traditional research methodologies.  Much of the theoretical basis comes from the field of statistics (Zumel & Mount, 2014), but the famous statistician William Cleveland argued that it is an interdisciplinary field that is much larger than statistics (Cleveland, XXX).  A major difference is that data science draws heavily from knowledge in the areas of computer science, software engineering, and information technology.  Unlike traditional academic research, data science is particularly well suited for tackling the _3 V’s_ of big data:  _volume_, _variety_, and _velocity_.  More specifically, in data-rich environments, researchers encounter an amount or volume of data that often exceeds the capacity of traditional database systems.  Data also come in a variety of formats, ranging from highly structured data contained in relational databases to massive stores of unstructured text-data culled from social media.  Many data opportunities go beyond the traditional approach of downloading a dataset and analyzing it with an off-the-shelf statistical package like SPSS, Stata, or SAS.  Now, data can be accessed in real time through application program interfaces (API), which brings along the challenge of _velocity_ — that is, data can be transmitted at a rate that grossly exceeds the computing power of desktop computers.  These are practical challenges that need to be addressed in order to maximize the potential of these new data.   

Another feature that distinguishes data science from traditional academic research is the focus on generating _actionable insights_ from data rather than theory building and hypothesis testing. In fact, data science is more interested in deriving useful insights from correlational patterns as opposed to uncovering causal mechanisms.  The focus is on the development of data products (Davenport, XXX), as opposed to reporting analysis.  Data products are tools or services that generate actionable insights from the data itself, including (but not limited to) automated reports or dashboards, recommendation systems, prediction algorithms, decision tools, and interactive visualizations that help people with non-technical background make sense of complex data.  Data science also values the philosophy of _openness_ or open science.  Thus, much effort is devoted to creating sustainable systems that make data as widely and freely accessible as possible, and developing and promoting the use of open source tools for managing and analyzing data.  The concept of reproducible research is a _rule_ rather than the _exception_.  This involves making both data and statistical code available alongside research reports.  This helps ensure the quality of the research, in addition to allowing researchers to make advances without having duplicate work that has already been done.  Data science is by no means incompatible with traditional academic research.  We believe that the tools, research strategies, and principles of data science are complementary to traditional ways of knowledge building in social work, offering many unique opportunities to improve the quality, efficiency, and impact of this area of research.  

### Overview of Current Study 

In the current study, we seek to utilize the opportunities of the data revolution and data science by applying a range of tools and strategies to construct an historical base of social work research.  In doing so we take on two of the 3 Vs of big data: volume due the amount of data collected and variety in terms of the multiple sources of that data.  The data are article records harvested from existing bibliographic databases hosted on ProQuest and EbscoHost.  Article records are already widely used in various bibliometric studies, particularly those that are intended to reveal publication networks within scientific communities.  It does so by creating links between co-authors, and then visually representing the edges and vertices of the network.  This is also done with article citation histories, to show influence over time.  In this study, we were interested in the various article meta-data that are common to many scientific article records.  For example, a single article record in psycINFO can have as many as XX pieces of meta-data including:  dates, journal titles, article titles, author names, author affiliations, full text abstracts, keywords, subject classification, methodology, location, populations, page numbers, and source of funding.  This is a rather large amount of information about a single paper, at least compared to the amount of information contained in article records before the availability of electronic storage and access.  Large collections of article records may contain potentially interesting trends in the research, assuming the selection of the article records were conducted in a way that allowed meaningful inferences to be derived. 

In the current study, we sought to capture every journal article record published in every social work journals over the in past quarter century (1989-2013) and to explore the various meta-data to uncover potentially interesting trends in the social work scholarship.  For example, we can reasonably infer estimates of the _size_  of social work scholarship by counting the number of unique article records.  And, growth of the field can be inferred by examining the number of article records over time.  Similarly, we examine the extent to which _team science_ has become part of the research practice, as this has implications for training of social work researchers.  We also use location meta-data to understand the extent to which different areas of the world have and have not been the focus of social work research.  Finally, we used topic modeling to extract topic areas that define the focus of social work research.  The execution of this research was grounded in the tools and values of data science.  Thus, all data were managed and analyzed using the open source software, R.  We also used an open source authoring system that integrates the statistical analysis with the text of the manuscript.  Along with this manuscript, the authors will also release the actual data and statistical code, thereby meeting the requirements of _reproducible research_, as defined by King ().  To our knowledge, this is the first time fully reproducible research (as defined by King, XXXX), has been published in a social work journal.    



Davenport: http://blogs.wsj.com/cio/2014/06/25/so-you-want-to-build-a-data-product/

http://www.nsf.gov/pubs/2015/nsf15544/nsf15544.htm

http://www.nsf.gov/news/news_summ.jsp?cntn_id=123607&org=NSF&from=news


# Methods

### Extraction of article records

Our list of social work journals was guided by the work of Hodge and Lacasse (2011).  More specifically, Hodge and Lacasse (2011) identified 84 disciplinary social work journals based on a variety of sources, including _An Author’s Guide to Social Work Journals_ (NASW Press, 1997), Thyer’s (2005) more recent listing of social work periodicals, and Genamics JournalSeek (http://journalseek.net/). Hodge and Lacasse (2011) examined the mission and aims of each journal, and eliminated journals that were specific to another field or had an had an inter-disciplinary focus.  For the current study, our primary search query included all journal titles in this list.

To help ensure comprehensive coverage of all possible core social work journals, we created a supplemental search query that extracted journal titles with any of the following terms:  

+ "social work"
+ "social welfare"
+ "social casework"
+ "social service" 
+ "human  service" 
+ "social development" 
+ "social environment"

Using this search queries, we extract journal article recrods from three major databases on the EbscoHost platform:  PsycINFO, Social Service Abstracts, and Social Work Abstracts.  Because of known gaps in indexing in databases (Holden, Barker, Kuppens, Rosenberg & LeBreton, 2014; Holden, Barker, Covert-Vail, Rosenberg, & Cohen, 2009), we also searched ProQuest, which linked to an additional 46 minor databases, including (but not limited to): ERIC, Sociological Abstracts and Worldwide Political Science Abstracts.  In every search, we used a filter in attempt to extract only article records that were classified as _journal articles_, thereby excluding other forms of scholarly communication that was not relevant to the current study (e.g., book reviews, editorials, obituaries, etc.).  We limited the timeframe to a 25 year period, from 1989 to 2013 (inclusive).  Articles published in years 2014 and 2015 were excluded because of delays in indexing.  Search results were exported in batches of article records based on the restrictions of the platform.  Article records were text files in a _generic bibliographic format_.  The article records contained various meta-data based on the database from which the article record was extracted.   Meta-data include (but are not limited to):  article title, journal title, publication year, author name(s), author affiliation(s), abstract, keywords, methodological classification, funding source, location of study, subject groups, digital object identifier (DOI), number of references, number of pages, etc.  These files were post-processed into a structured database using a set of scripts written in the R statistical programming language.  The initial search resulted in `r pretty(nrow(article.count.initial))` article records from `r journal.count.initial`.

The search queries were purposefully specified to be overly inclusive to ensure full coverage of all possible social work journals contained in the major and minor databases.  Additionally, a visual inspection of the article records revealed problems in the original indexing of articles that requires subsequent data cleaning.  Thus, our data cleaning procedures focused separately on journal titles and article records.


### Data cleaning: Journal titles

The first step of data cleaning involved fixing journal title names due to discrepant errors in indexing. For example, _Journal of Gay and Lesbian Social Services_ was indexed as a journal separate from _Journal of Gay & Lesbian Social Services_ (use of "and" vs. "&").  Other examples involved journals indexed with and without subtitles, or journals with and without the word "The" at the beginning of the title.  Additionally, some journals changed titles over their history.  For example, _Journal of Technnology in Human Services_ is formerly known as _Computers in Human Services_.  In these situations, the former titles were merged with the current titles.  

Many journals from allied health disciplines used one of the supplemental search terms in special editions, which were also included in the journal title.  Thus, many non-social work journals were captured in the extraction process.  To resolve this issue, we created a list of all journal titles that were not part of the core list defined by Hodge and Lacasse (2011).  Study authors reviewed these journal titles and discussed whether each candidate title should be retained or excluded.  When disagreements occurred, the study authors reviewed the mission and aims of the journals, names of editorial board members, and focus of the articles.  A consensus was reached on all journal titles to be excluded and retained.   After these procedures, the number of article records and journal titles was reduced to `r pretty(nrow(article.count.journalsCleaned))` and `r journal.count.journalsCleaned`.  

### Data cleaning:  Article records

Our search procedures involved the use of three major databased and 46 minor databases, which resulted in some duplication of article records.  Thus, a matching algorithm was constructed to identify and remove all duplicate article records. As previously noted, the filtering of only journal articles was not successful due to errors in the original indexing.  Thus, other scholarly communications were captured in the extractio process.  These other scholarly communications were readily identified due to specific patterns in the titles, such as the terms "Book Review," "From the Editor," and "Obituary."  Given the number of article records in the database, it was not feasbile to manually extract these recrods.  Instead, we created a separate database with representative examples of article records that we wanted to extract.  We also added representative examples of article records to be retained.  We then wrote a series of _regular expressions_ to extract the problematic article records.  Regular expressions are a sequence of characters that form search patterns, allowing extraction of records that match the specified pattern.  In other words, it is a more sophisticated implementation of the search function common to all major word processors.  We tested our regular expressions on the test database, achieving > 95% accuracy with regard to retaining and extracting article records.  These regular expressions were then applied to the full database.  We then excluded article records if they were missing essential meta-data, including author name(s), journal title, article title, and publication field.  As an additional requirement, we required all articles to be $\geq 3$ pages in length.   Finally, we eliminated all social work journals with total article counts of < 10 for the entire 25 year window (_Issues in Social Work Education_, _Maatskaplike Werk/Social Work_, _Pediatric Social Work_, and _Critical Social Work_).  These cleaning procedures resulted in a final database of `r pretty(articles.final)` articles and `r journals.final` journals.  

### Data quality checks

We performed a variety of checks on the final database.  Because the size of the database, it was not feasible to manually inspect all records.  Thus, we randomly sampled article records from the database and cross-checked the data we procured with the information listed with the journal's homepage.  We also inspected a scatter plot of the number of article records for each journal by the number of years the journal appeared within the 25 window timeframe.  Journals that exhibited significant deviation from the regression line were inspected to ensure that our cleaning procedures did not systematically exclude article records or retain other scholarly communications.  All observed discrepancies were verified as differences in journals actual publication output.  Finally, we also made checks of number of journal article records for various journals indexed in the _Web of Science_.  Although the counts were close -- i.e., within 10% -- similar indexing problems observed from our record extraction were also observed in the _Web of Science_ database, such as misclassification of book reviews as journal articles.  Thus, we are unable to quantify the actual amount of error in our database is due to problems in indexing.  At the same time, we are relying on the same source of data that social work researchers use to inform their work.  Too much cleaning and post-processing of the search results can give rise to validity issues because social work researchers are using an information source that contain the errors we have attempted to eliminate.  The issue of reliability and validity are given further attention in the discussion section of this study.  

# Results

\newpage

```{r finalArticleListecho=FALSE, comment=NA}
print.data.frame(final.list, row.names=FALSE)
```


\newpage

```{r}
grid.arrange(plot.article.count, plot.journal.count, ncol=2)
```

\newpage

```{r}
plot.PhD
```

\newpage

```{r}
plot.author.count
```

\newpage

# Appendix A

Articles on Hodge and Lacasse (2011) but not in the current study.  

```{r appendix_Print, eval=TRUE}
print(sort(hodge.swHistory.diff))
```
