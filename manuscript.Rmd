--
title: 'Examining a Quarter Century of Publishing Trends in Social Work Research:
  A Data Science Perspective'
author: ''
output: pdf_document
---

`r knitr::opts_chunk$set(echo=FALSE, message=FALSE, warning=FALSE, comment=NA)`

```{r colorThemes}
# http://colorbrewer2.org/?type=diverging&scheme=PuOr&n=4

cp <- c("#fdb864", "#e66101", "#b2abd2", "#5e3c99" )
```

```{r objectMakerProQuest, warning=FALSE, message=FALSE, eval=FALSE, results='hide', cache=TRUE}
# Clear workspace
rm(list=ls())

# Read BWR functions for Mac OS
source("/Users/beperron/Git/BibWrangleR/functions/proquestBWR.R")

# Set the path where original raw data are stored
setwd("/Users/beperron/Git/SocialWorkResearch/Data")

# Set the working directory to store files created by BWR functions
path <- "/Users/beperron/Git/SocialWorkResearch/Data/ProQuest"

# Wrangle the data with the BWR function suite

proQuestBWR.f(csv=FALSE, path=path)
save(proQuestBWR.df, file = "ProQuest.R")
```

```{r objectMakerEbscoHost,warning=FALSE, message=FALSE, eval=FALSE, results='hide', cache=TRUE}
# Clear workspace
rm(list=ls())

# Read BWR functions for Mac OS
source("/Users/beperron/Git/BibWrangleR/functions/ebscoBWR.R")

# Set the path where original raw data are stored
setwd("/Users/beperron/Git/SocialWorkResearch/Data")

# Set the working directory to store files created by BWR functions
path <- "/Users/beperron/Git/SocialWorkResearch/Data/ebscoFULL"

# Wrangle the data with the BWR function suite

ebscoBWR.f(csv=FALSE, path=path)
save(ebscoBWR.df, file = "ebscoFULL.R")
```

```{r BEPinitialization, eval=TRUE}
rm(list=ls())  

#load("/Users/beperron/Git/SocialWorkResearch/socialWorkHistory.Rdata")

#Load required packages
library(ggplot2)
library(dplyr)
library(gridExtra)
```

```{r BGVinitialization, eval=FALSE}
rm(list=ls())  
load("/Users/SSW/Documents/GitHub/SocialWork100Years/socialWorkHistory.Rdata")

# Load required packages
library(ggplot2)
library(dplyr)
library(gridExtra)
```

```{r cleaningFunctions, comment=NA, echo=FALSE, eval=TRUE, cache=TRUE}
# Read data files
options(stringsAsFactors = FALSE)
load("/Users/beperron/Git/SocialWorkResearch/Data/ebscoFULL.R")
load("/Users/beperron/Git/SocialWorkResearch/Data/proQuest.R")
hodge <- readLines("/Users/beperron/Git/SocialWorkResearch/search_specifications/hodge.txt", n = -1)

source("/Users/beperron/Git/SocialWorkResearch/functions/combineData.R")
source("/Users/beperron/Git/SocialWorkResearch/functions/cleaningJournals.R")
source("/Users/beperron/Git/SocialWorkResearch/functions/shortArticlesRevised.R")
source("/Users/beperron/Git/SocialWorkResearch/functions/titleMatching.R")
source("/Users/beperron/Git/SocialWorkResearch/functions/otherDocuments.R")
source("/Users/beperron/Git/SocialWorkResearch/functions/fewArticles.R")
source("/Users/beperron/Git/SocialWorkResearch/functions/pretty.R")

combineData.f()
cleaningJournals.f()
shortArticles.f()
titleMatching.f()
otherDocuments.f()
fewArticles.f()
#save.image(file = "socialWorkHistory.Rdata")
```

```{r tableArticleSummary, eval=TRUE, cache=TRUE}
unique.titles <- filter(full.df, attributes == "journal") %>%
    group_by(record) %>%
    summarise(N = n())

articles.titleMatching <- length(which(full.df$attributes == "article"))

journals.titleMatching <- filter(full.df, attributes == "journal") %>%
    summarise(Unique = n_distinct(record))

articles.otherDocuments <- length(which(full.df$attributes == "article"))

journals.otherDocuments <- filter(full.df, attributes == "journal") %>% 
    summarise(Unique = n_distinct(record))

articles.final <- length(which(full.df$attributes == "article"))

journals.final <- filter(full.df, attributes == "journal") %>% 
    summarise(Unique = n_distinct(record))



unique.titles <- filter(full.df, attributes == "journal") %>%
    group_by(record) %>%
    mutate(Hodge.list = ifelse(record %in% hodge == TRUE, 1, 0)) %>%
    summarise(N = n(), Hodge.list = max(Hodge.list)) %>%
    mutate(Hodge = ifelse(Hodge.list == 1, "Y", "N"))

n.so.yr <- filter(full.df, attributes == "journal" | attributes == "pubYear")

n.so <- filter(full.df, attributes == "journal") %>% mutate(title = record) %>% 
    select(-attributes, -record)

n.yr <- filter(full.df, attributes == "pubYear") %>% mutate(year = record ) %>% 
    select(-attributes, -record)


```


```{r plotSmallMultiples, cache=FALSE}
# Remove 2014 & 2015 

# Create an initial data frame from HD
sm.df <- left_join(n.so, n.yr) %>%
    group_by(title, year) %>%
    summarize(N = n()) %>%
    mutate(Overall.N = sum(N), year = as.numeric(year)) %>%
    ungroup() %>%
    arrange(desc(Overall.N), title) %>%
    rename(Year = year, Journal= title) 

# Read Scopus database
scopus.df <- read.csv("/Users/beperron/Git/SocialWorkResearch/Data/scopus.csv")
scopus.df2 <- read.csv("/Users/beperron/Git/SocialWorkResearch/Data/scopus_OldHarvest.csv")


scopus.df$Year <- as.numeric(scopus.df$Year)
scopus.df <- filter(scopus.df, Year <= 2013)

# Join HD with Scopus and join abbreviation names
sm.scopus.merged <-full_join(sm.df, scopus.df, by = c("Journal", "Year")) %>%
    select(Year, Journal, N, Overall.N, Documents)




 sm.scopus.merged.table <- group_by(sm.scopus.merged, Journal) %>%
     summarize(DataBase.N = sum(N, na.rm=TRUE), Scopus.N = sum(Documents, na.rm=TRUE)) %>%
     arrange(desc(DataBase.N))


 abb <- read.csv("/Users/beperron/Git/SocialWorkResearch/Data/master.csv")
 abb$ID <- 1:nrow(abb)
 sm.scopus.merged <- full_join(sm.scopus.merged, abb, by = "Journal") %>% 
     select(-X) %>%
     mutate(Documents = ifelse(Documents == 0, NA, Documents))
 
 
 # Re-order the factor by Overall.N
 title <- sm.scopus.merged %>% arrange(desc(Overall.N)) %>% 
     distinct(Abbrev) %>% filter(!is.na(Abbrev)) %>% distinct()
 
 title.order = title$Abbrev
 
 sm.scopus.merged$Abbrev <- factor(sm.scopus.merged$Abbrev, levels = (title.order))
 sm.scopus.merged <- group_by(sm.scopus.merged, Journal) %>%
     mutate(Overall.Scopus = sum(Documents, na.rm=TRUE)) %>%
     ungroup()
# 
 # Create labels for the facets
 Overall.N <- distinct(sm.scopus.merged, Abbrev) 
 lab1 <- gsub(" ", "", paste("N=", pretty(Overall.N$Overall.N), sep=""))
 lab2 <- gsub(" ", "", paste("N=", pretty(Overall.N$Overall.Scopus), sep=""))
 lab <- paste(lab1, lab2, sep="\n")
 xpos = rep(1999, 79)
 ypos = rep(200, 79)
 ldata <- data.frame(xpos, ypos, lab, Overall.N$Abbrev) %>% 
     rename(Abbrev = Overall.N.Abbrev) 
 
 ldata <- na.omit(ldata)
 
 
 titles.reduced <- unique(sm.scopus.merged$Journal)
 titles.reduced.1 <- titles.reduced[1:40]
 titles.reduced.2 <- titles.reduced[41:length(titles.reduced)] 
 cp <- c("#fdb864", "#e66101", "#b2abd2", "#5e3c99" )
 
 library(grid)
 library(gridExtra)
# Plotting function    
# 
 plotSmallMultiples.1 <-  ggplot(sm.scopus.merged[sm.scopus.merged$Journal %in% titles.reduced.1, ],  aes(Year, N)) +
     geom_line(colour="#e66101") +
     geom_line(data = sm.scopus.merged[sm.scopus.merged$Journal %in% titles.reduced.1, ], aes(Year, Documents), colour = "#533c99", alpha = .80) + 
     facet_wrap(~ Abbrev) + 
     theme_bw() +
     theme(strip.text.x = element_text(size = 8)) + 
     theme(legend.position = "bottom", 
     panel.grid.minor = element_blank(), 
     axis.text.x = element_text(angle=45, hjust=1, size =8)) + 
     geom_text(data = ldata[1:40, ], aes(x =xpos, y = ypos, label = lab), size =2.5) +
     ylim(c(0,225)) +
     scale_x_continuous(breaks=seq(1989, 2015, 8)) +
     annotate("segment", x = 1989, xend = 1992, y = 215, yend = 215, colour = "#e66101") + 
     annotate("segment", x = 1989, xend = 1992, y = 190, yend = 190, colour = "#533c99")

plotSmallMultiples.2 <- ggplot(sm.scopus.merged[sm.scopus.merged$Journal %in% titles.reduced.2, ],  
     aes(Year, N)) +
     geom_line(colour="#e66101") +
     geom_line(data = sm.scopus.merged[sm.scopus.merged$Journal %in% titles.reduced.2, ], 
     aes(Year, Documents), colour = "#533c99", alpha = .80) + 
     facet_wrap(~ Abbrev) + 
     theme_bw() +
     theme(strip.text.x = element_text(size = 8)) +  
     theme(legend.position = "bottom", 
     panel.grid.minor = element_blank(), 
     axis.text.x = element_text(angle=45, hjust=1, size =8)) + 
     geom_text(data = ldata[41:79, ], aes(x =xpos, y = ypos, label = lab), size =2.5) +
     ylim(c(0,225)) +
     scale_x_continuous(breaks=seq(1989, 2015, 8)) +
     annotate("segment", x = 1989, xend = 1992, y = 215, yend = 215, colour = "#e66101") + 
     annotate("segment", x = 1989, xend = 1992, y = 190, yend = 190, colour = "#533c99")
 

# x1.grob <- ggplotGrob(plotSmallMultiples.1)
# x2.grob <- ggplotGrob(ggplot(cars, aes(speed, dist)) + geom_point())
# 
# library(gtable)
# plot1 <- gtable_filter(x1.grob, 'panel', trim=F)$layout
# pg <- gtable_add_grob(x1.grob, x2.grob, t = max(plot1$t), l = max(plot1$l))
# grid.newpage()
# grid.draw(pg)
```



```{r plotJournalArticle, cache=FALSE}
#=======================================
# Journals
#=======================================
library(gridExtra)

n.journals.year <- filter(full.df, attributes == "journal") %>%
    select(articleID, record)

n.year <- filter(full.df, attributes == "pubYear") %>%
    select(articleID, record)

n.journals.year <- left_join(n.journals.year, n.year, by = "articleID", copy=TRUE) %>%
    group_by(record.y) %>%
    summarise(Journal.count = n_distinct(record.x)) %>%
    mutate(record.y = as.numeric(record.y))

plot.journal.count <- ggplot(n.journals.year, aes(record.y, 
    y = Journal.count, group=1)) + 
    theme_bw() + 
    geom_line(colour=cp[4], size = 1) +
    theme(axis.text.x = element_text(size=8), 
    title = element_text(size = 10), plot.title=element_text(size=10)) + 
    xlab("Year") + 
    ylab("Number of Journals") + 
    scale_y_continuous(breaks = c(seq(0, 100, 5)), limits = c(20, 70)) +
    scale_x_continuous(breaks=seq(1989, 2015, 8)) +
    theme(axis.title.x = element_text(vjust=-0.5), axis.title.y = element_text(vjust=.75))

#=======================================
# Articles
#=======================================

articles.df <- left_join(n.so, n.yr) %>%
    group_by(year) %>%
    summarize(N = n()) %>%
    mutate(year = as.numeric(year), CumSum = cumsum(N)) %>%
    ungroup() %>%
    arrange(desc(N), year) %>%
    rename(Year = year) 


plot.article.count <- ggplot(articles.df, aes(Year, N, group =1)) + 
    theme_bw() + 
    geom_line(colour=cp[2], size = 1) +
    theme(axis.text.x = element_text(size=8), 
    title = element_text(size = 10), plot.title=element_text(size=10)) + 
    xlab("Year") + 
    ylab("Number of Articles") + 
    scale_x_continuous(breaks=seq(1989, 2015, 8)) +
    scale_y_continuous(breaks = c(seq(0, 2200, 250)), limits= c(500, 2100)) +
    theme(axis.title.x = element_text(vjust=-0.5), axis.title.y = element_text(vjust=.75))
```    



```{r plotAuthorCount, eval=TRUE, cache=FALSE}
year.df <- full.df %>%
    filter(attributes == "pubYear") %>%
    select(id = articleID, pubYear = record)

authors.df <- full.df %>%
    filter(attributes == "author") %>%
    select(id = articleID, author = record)

n_authors <- authors.df %>%
    group_by(id) %>%
    summarise(n=n())

n_authors <- n_authors%>% 
    left_join(year.df) %>%
    group_by(pubYear) %>%
    mutate(n = as.numeric(n)) %>%
    summarise(median.n = median(n),
        average.n = mean(n),
        min.n = min(n),
        max.n = max(n),
        std.dev  = sd(n) )

n_authors2 <- n_authors%>% 
   select(median.n, average.n, std.dev) %>%
   rename(Median = median.n, Average = average.n, Standard_Deviation =std.dev)


n_authors2$year <- c(1989:2013)


cp <- c("#fdb864", "#e66101", "#b2abd2", "#5e3c99" )

n_authors2.melted <- reshape2::melt(n_authors2, id.vars = c("year", "Standard_Deviation"))
n_authors3.melted <- filter(n_authors2.melted, variable == "Average")

plot.author.count <- ggplot(n_authors2.melted, aes(x = year, y = value, group=variable, colour = variable)) + 
    theme_bw() + 
    geom_line() +
    geom_point(data = n_authors3.melted, aes(size = Standard_Deviation), colour = "#e66101") + 
    #theme(axis.text.x = element_text(angle = 45, hjust = 1), 
    #title = element_text(size = 10)) +  
    scale_x_continuous(breaks=c(seq(1989, 2013, 4))) +
    scale_y_continuous(limits = c(.75,2.5)) +
    ylab("Number of Authors") +
    xlab("Year") +
    labs(size = "Standard deviation") +
    labs(colour = "") + 
    scale_colour_manual(values = c("#fdb864", "#e66101")) +
   theme(axis.title.x = element_text(vjust=-0.5), axis.title.y = element_text(vjust=.75))

```


```{r pagePlot, eval=TRUE, cache=FALSE}
cp <- c("#fdb864", "#e66101", "#b2abd2", "#5e3c99" )
pages.tmp <- full.df %>% filter(attributes == "pages")
years.tmp <- full.df %>% filter(attributes == "pubYear")
pages.df <- left_join(pages.tmp, years.tmp, by = "articleID") %>% 
    select(record.y, record.x) %>%
    rename(years = record.y, pages = record.x) %>%
    mutate(pages = as.numeric(pages), years = as.numeric(years)) %>%
    group_by(years) %>%
    summarise(
        Median = median(pages, na.rm=TRUE), 
        Average = mean(pages, na.rm=TRUE),
        Std.Pages = sd(pages, na.rm=TRUE))



pages.df$years <- c(1989:2013)

pages.df.melted <- reshape2::melt(pages.df, id.vars = c("years", "Std.Pages"))

pages.df2.melted <- filter(pages.df.melted, variable == "Average")






page.plot <- ggplot(pages.df.melted, aes(x = years, y = value, group=variable, colour = variable)) + 
    geom_line() +
    geom_point(data = pages.df2.melted, aes(size = Std.Pages), colour = "#5e3c99") + 
    theme_bw() + 
    theme(title = element_text(size = 10)) +  
    scale_x_continuous(breaks=c(seq(1989, 2013, 4))) +
    ylab("Number of Pages") +
    xlab("Year") +
    labs(size = "Standard deviation") +
    labs(colour = "") + 
    scale_colour_manual(values = c("#b2abd2", "#5e3c99"))  +
    theme(axis.title.x = element_text(vjust=-0.5), axis.title.y = element_text(vjust=.75))

```

```{r plotStacked, cache=FALSE}
abstract.df <- filter(full.df, attributes == "abstract")
year.df <- filter(full.df, attributes == "pubYear")
journal.df <- filter(full.df, attributes == "journal")
abstract.df <- left_join(year.df, abstract.df, by = "articleID")
abstract.df <- left_join(abstract.df, journal.df, by = "articleID")
abstract.df <- abstract.df %>% select(-attributes.x, -attributes.y, -attributes)
colnames(abstract.df) <- c("articleID", "year", "abstract", "journal")
abstract.df$abstract <- gsub("[[:punct:]]", "", abstract.df$abstract)
abstract.df$abstract <- tolower(abstract.df$abstract)

ebp.df <- abstract.df[grep("evidence\\s?based\\spractices?|ebp|empirically\\s?supported\\streatments?", abstract.df$abstract), ]
ebp.df$studyType <- rep("ebp", nrow(ebp.df))

meta.df <- abstract.df[grep("meta\\s?analysis", abstract.df$abstract), ]
meta.df$studyType <- rep("meta", nrow(meta.df))

rct.df <- abstract.df[grep("randomi[s|z]ed.*trial|[^non]experimental\\sdesign|rct|controlled\\sclinical\\strial|cct", abstract.df$abstract), ]
rct.df$studyType <- rep("rct", nrow(rct.df))

systematicReview.df <- abstract.df[grep("systematic\\s?review", abstract.df$abstract), ]
systematicReview.df$studyType <- rep("sr", nrow(systematicReview.df))

quasi.df <- abstract.df[grep("cohort\\sstudy|case\\scontrol\\sstudy|quasi\\s?experimental|non\\s?experimental", abstract.df$abstract), ]
quasi.df$studyType <- rep("quasi", nrow(quasi.df))

evidence.df <- rbind(meta.df, rct.df, systematicReview.df, 
     quasi.df) 

evidence.df$studyType <- factor(evidence.df$studyType, labels= c("Meta-analysis", "Quasi-experimental", "RCT", "Systematic review"))

evidence.df$studyType <- factor(evidence.df$studyType, 
    levels(evidence.df$studyType)[c(1,4,3,2)])


evidence.summary <- select(evidence.df, -abstract) %>% 
    group_by(year, studyType) %>%
    summarize(N = n()) %>%
    group_by(studyType) %>%
  mutate(cumulative = cumsum(N))




cp <- c("#b2abd2", "#5e3c99", "#fdb864", "#e66101" )
evidence.summary <- ungroup(evidence.summary)
evidence.summary$year <- as.numeric(evidence.summary$year)
#evidence.summary$studyType <- as.factor(evidence.summary$studyType)


abstractsYear.df <- group_by(abstract.df, year) %>% 
    summarize(N.abstracts = n()) %>%
    mutate(year = as.numeric(year))

evidence.summary2 <- full_join(evidence.summary, abstractsYear.df, 
    by = "year") %>%
    mutate(studyProp =N / N.abstracts)



year.labs <- as.character(seq(1989,2013, 4))  

stacked.df <- as.data.frame(xtabs(N~year+studyType, evidence.summary))


stacked.plot <- ggplot(stacked.df, 
    aes(x = as.numeric(year), y = Freq, fill=studyType)) + 
    geom_area(alpha=.60, colour = "black", size = .2) +
    theme_bw() +
    scale_fill_manual(values=cp, 
        breaks = rev(levels(evidence.summary$studyType)))+
    scale_x_continuous(breaks = seq(0,24, 4), labels = year.labs) + 
    theme(  legend.position=c(.25,.80),
#         legend.background = element_blank(), 
         legend.key = element_blank()   ) + 
    ylab("Number of Articles") + 
    xlab("Year") + 
    labs(fill = "") +
    scale_y_continuous(breaks = seq(0,90, 10)) +
    theme(axis.title.x = element_text(vjust=-0.5), axis.title.y = element_text(vjust=.75))


```

```{r plotProportions, cache=FALSE}
df <- evidence.summary2

evidence.plot <- ggplot(na.omit(df), aes(year, studyProp, colour = studyType, 
    group = studyType), na.rm=TRUE) + 
    geom_smooth(aes(fill=studyType), alpha = .40, colour = "dimgray") + facet_wrap(~studyType) + 
    theme_bw() +
    scale_fill_manual(values = cp) +   
    scale_color_manual(values = cp) + 
    theme(legend.position = "none") + 
    ylab("Proportion") + 
    geom_point(colour = "black", alpha = .60) + 
    annotate("segment", x = 1989, y = 0, xend = 2013, yend = 0, alpha = .6, 
        linetype= "solid") +
    annotate("segment", x = 1989, y = 0, xend = 1989, yend = .0175, alpha = .6, 
        linetype = "solid") +
    annotate("segment", x = 2013, y = 0, xend = 2013, yend = .0175, alpha = .6,
             linetype = "solid") +
    scale_x_continuous(breaks = seq(1989, 2014, 4)) + 
     theme(axis.text.x = element_text(angle = 45, hjust = 1)) + 
    scale_y_continuous(breaks = seq(0, .03, .005))
``` 


```{r plotClevelandAbsolute}
evidence.summary3 <- select(evidence.df, -abstract) %>% 
    group_by(journal, studyType) %>%
    summarize(N = n()) 

evidence.summaryOverall <- select(abstract.df, -abstract) %>%
    group_by(journal) %>%
    summarize(Overall.N = n())

evidence.journals <- full_join(evidence.summary3, evidence.summaryOverall, by = "journal") %>%
    mutate(JN.prop = N/Overall.N) %>% group_by(journal) %>%
    mutate(Overall.N.Strength = sum(N))
    
evidence.journalsOverall <- evidence.journals %>% group_by(journal) %>%
    mutate(Overall.N.Strength = sum(N))

df <-  select(evidence.journalsOverall, studyType, N, Overall.N.Strength) %>%
    rename(N.sort = Overall.N.Strength)

df1 <- df

df1.summarized <- df1 %>% group_by(journal) %>% 
    summarize(EBP.counts = sum(N)) %>% 
    arrange(desc(EBP.counts)) %>%
    slice(1:15) 

df1.reduced <- na.omit(df1[df1$journal %in% df1.summarized$journal, ])
df1.reduced <- ungroup(df1.reduced)

df1.reduced <- df1.reduced %>% mutate(journal = paste(journal, N.sort, sep = "  "))

#df1$studyType <- factor(df1$studyType)
j.order <- unique(df1.reduced$journal[order(df1.reduced$N.sort, decreasing = FALSE)]) 
df1.reduced$journal <- factor(df1.reduced$journal, levels = j.order)

Text1 <- textGrob("Overall N")
library(gridExtra)


journal.evidence.plot <- ggplot(df1.reduced, 
    aes(x = N, y= journal, group = journal, colour = studyType, shape= studyType)) + 
    geom_segment(aes(yend = journal), xend=0, colour="grey90") +
    geom_point(size = 2) + 
    scale_color_manual(values = cp) + 
    theme_bw() +
    #facet_grid(.~studyType) + 
    theme(panel.grid.major.y = element_blank(), legend.position = c(.75, .5), 
    legend.title = element_blank(), legend.text = element_text(size = 8)) + 
    xlim(0, 600) + 
    ylab(NULL) +  
    theme(axis.text.x = element_text(hjust = 1, size = 8), axis.ticks = element_blank()) + 
    theme(axis.text.y = element_text(size = 8) ) + 
    scale_x_continuous(breaks = seq(0, 60, 10)) +
    scale_shape_manual(values=c(24, 23, 21, 25))

journal.evidence.plot <- arrangeGrob(journal.evidence.plot, main = textGrob("Overall N", gp=gpar(cex=.7, font=2), vjust = 2, hjust = 1.25))
```


```{r plotPareto}
pareto.df <- df1 %>% group_by(journal) %>% 
    summarize(EBP.counts = sum(N)) %>% 
    arrange(desc(EBP.counts)) %>%
    ungroup() %>%
    mutate(overallCount = sum(EBP.counts, na.rm=TRUE)) %>%
    mutate(percentTotal = (EBP.counts/overallCount) *100) %>%
    mutate(cumSum = cumsum(percentTotal)) %>%
    slice(1:16)
    
df1.reduced <- na.omit(df1[df1$journal %in% pareto.df$journal, ])
df1.reduced <- ungroup(df1.reduced)
df1.reduced <- full_join(df1.reduced, pareto.df, by = "journal")


#df1.reduced <- df1.reduced %>% mutate(journal = paste(journal, N.sort, sep = "  "))

j.order <- unique(df1.reduced$journal[order(df1.reduced$N.sort, decreasing = FALSE)]) 

df1.reduced$journal <- factor(df1.reduced$journal, levels = j.order)


df1.unique <- group_by(df1.reduced, journal) %>% slice(1)
df1.unique$journal <- factor(df1.unique$journal, levels = j.order)

journal.evidence.plot <- ggplot(na.omit(df1.reduced), aes(x = journal, y = N)) + 
    geom_bar(aes(fill = studyType), stat = "identity") +
    coord_flip() +
    theme_bw() + 
    geom_text(data = df1.unique, aes(y = EBP.counts+7, x = journal, label = EBP.counts), size = 3) +
    labs(x = "Journal", y = "N") +
    ylim(c(0, 150)) +
    scale_fill_manual(values = cp) 
```

```{r plotClevelandRelative}

proportionsEBP.df <- df1.reduced %>% 
    group_by(journal) %>% 
    slice(1) %>%
     mutate(Journal = as.character(journal)) %>%
    ungroup() %>%
    select(-studyType, -N, -N.sort, -journal, -overallCount, -percentTotal, -cumSum) 
   
proportionEBP2.df <- full_join(proportionsEBP.df, sm.scopus.merged.table, by = "Journal")  %>% 
    mutate(journalProp = EBP.counts / DataBase.N) %>%
    arrange(desc(journalProp)) 






target.journals <- df1.reduced 
target.journals$journal <- gsub("\\s+\\d+", "", target.journals$journal)
EBP.df <- left_join(target.journals, evidence.journals, by = c("journal", "studyType")) %>%
    group_by(journal) %>%
    slice(1) %>%
    ungroup %>%
    mutate(Proportion = N.sort/Overall.N) %>%
    rename(Absolute = N.sort)


EBP.df  <- within(EBP.df, quartile <- as.integer(cut(Overall.N, quantile(Overall.N, probs=0:4/4, na.rm=TRUE), include.lowest=TRUE)))



ggplot(EBP.df, aes(Proportion, journal, size = Overall.N, colour=as.factor(quartile))) + 
    theme_bw() + 
    geom_point()


View(df1.reduced)





evidence.journals <- full_join(evidence.summary3, evidence.summaryOverall, by = "journal") %>%
    group_by(journal) %>%
    mutate(Overall.N.Strength = sum(N)) %>%
    select(-studyType, N) %>%
    distinct(journal, Overall.N, Overall.N.Strength) %>%
    mutate(Prop = Overall.N.Strength / Overall.N) 


df2 <- data.frame(evidence.journals)

j.order <- unique(df2$journal[order(df2$Overall.N.Strength, decreasing = FALSE)]) 
df2$journal <- factor(df2$journal, levels = j.order)

df2  <- within(df2, quartile <- as.integer(cut(Overall.N.Strength, quantile(Overall.N.Strength, probs=0:4/4, na.rm=TRUE), include.lowest=TRUE)))

df2 <- df2 %>% arrange(desc(Prop)) %>% slice(1:15)

EBP.plot <- ggplot(na.omit(df2), aes(x = Prop, y= journal, group = journal)) + 
    geom_segment(aes(yend = journal), xend=0) +
    geom_point(aes(size = Overall.N.Strength)) + 
    theme_bw() +
    theme(panel.grid.major.y = element_blank(), legend.position = c(.62, .65)) +
    ylab("Journal") +
    xlab("Proportion") +
    labs(size = "Total number of \nEBP articles published") 
```


```{r appendix_A_intersection}
unique.swHistory <- filter(full.df, attributes == "journal") %>%
    distinct(record)

hodge.swHistory.intersect <- intersect(hodge, unique.swHistory$record)

hodge.swHistory.diff <- setdiff(hodge, unique.swHistory$record)

```

### Data Revolution

It is estimated that 90% of all the data in human history has been generated in the past two years, and the amount of data continues to grow by roughly 2.5 quintillion bytes every day [(IBM, nd-a)](http://www-01.ibm.com/software/data/bigdata/what-is-big-data.html).  We have unprecedented opportunities for deriving actionable insights from these data to advance the human condition.  The significance continues to gain the attention of national and international organizations.  For example, the Independent Expert Advisory Group (IEAG) of the United Nations prepared a report entitled, _A World That Counts:  Mobilising the Data Revolution for Sustainable Development_ [(IEAG, 2014)](http://www.undatarevolution.org/report/).  The IEAG (2014) argues that "Data are the lifeblood of decision-making and the raw material for accountability.  Without high-quality data providing the right information on the right things at the right time; designing, monitoring and evaluating effective policies becomes almost impossible" (p. 2).  The United States federal government established an open data initiative, motivated by the awareness that data can have the greatest impact when it is "accessible, discoverable, and usable" [(www.data.gov)](www.data.gov).  Data from all levels of government that is made completely open can lead to "cost savings, efficiency, fuel for business, improved civic services, informed policy, performance planning, research and scientific discoveries, transparency and accountability, and increased public participation in the democratic dialogue" (www.data.gov).  At the time of preparing this article, over 124,000 data sets were freely available through www.data.gov among a diverse set of topic areas:  agriculture, business, climate, consumers, ecosystems, education, energy, finance, health, local government, manufacturing, ocean studies, public safety, and science and research.  

In 2012, the NIH established the Big Data to Knowledge (BD2K) initiative, recognizing the potential of data to advance our understanding of human health and disease.  This initiative was designed to support four broad goals:  1) to facilitate use of biomedical digital assets by making them discoverable, accessible, and citable; 2) to conduct research and develop methods, software and tools to analyze biomedical big data; 3) to enhance training in the development of use and tools necessary for biomedical big data science; and 4) to support a data ecosystem that accelerates discovery as part of the digital enterprise (NIH).  At roughly the same time, the National Science Foundation (NSF) created new initiatives to harness the potential of the data revolution, arguing that "data are motivating a profound transformation in the culture and conduct of scientific research in every field of science and engineering ... American scientists must rise to the challenges and seize the opportunities afforded by this new, data-driven revolution" (Suresh, XXX).  The programs of the NSF are organized around capacity building and innovative applications.  Capacity building activities focus on developing fundamental theories, techniques, methodologies and technologies that are broadly applicable to big data problems.  Innovative applications relate to specific domains advancing innovative ideas that provide solutions “with potential for a broader impact on data science and its applications.” 

### Data Science and Social Work

We are indeed living in a data revolution.  The relevance to social work research is clear — we have the opportunity to study human behavior and other social phenomena in real time, with vastly improved reliability in our measurements, and at scale.  Data comes from a wide range of sources, including (but not limited to) social media interactions, electronic medical records, audio and video recordings, click stream data, transaction data, web logs, embedded sensors, and GPS.  New tools allow blending of data sets, unique ways to analyze data, and methods of visualizations that can produce insights limited only by one's imagination.  As stated by the NIH, "lack of appropriate tools, poor data accessibility, and insufficient training, are major impediments to rapid translational impact" in this new era of data.  It is essential that the field of social work fully embraces the data revolution and considers the extent to which the existing research infrastructure and training is preparing both social work researchers and providers to be effective and efficient in a data rich world.  The potential of the data revolution is reflected in the current _Grand Challenge_ entitled, “Harness technology for social good.”  However, we believe that the data revolution goes beyond this single _Grand Challenge_ as it provides the opportunity to be informative in all areas of social work research.  

Data science serves as a useful framework for broadening the existing research infrastructure and training opportunities within social work.  Data science is similar to traditional academic research, as it includes the key roles of substantive expertise and traditional research methodologies.  Much of the theoretical basis comes from the field of statistics (Zumel & Mount, 2014), but the famous statistician William Cleveland argued that it is an interdisciplinary field that is much larger than statistics (Cleveland, XXX).  A major difference is that data science draws heavily from knowledge in the areas of computer science, software engineering, and information technology.  Unlike traditional academic research, data science is particularly well suited for tackling the _3 V’s_ of big data:  _volume_, _variety_, and _velocity_.  More specifically, in data-rich environments, researchers encounter an amount or volume of data that often exceeds the capacity of traditional database systems.  Data also come in a variety of formats, ranging from highly structured data contained in relational databases to massive stores of unstructured text-data culled from social media.  Many data opportunities go beyond the traditional approach of downloading a dataset and analyzing it with an off-the-shelf statistical package like SPSS, Stata, or SAS.  Now, data can be accessed in real time through application program interfaces (API), which brings along the challenge of _velocity_ — that is, data can be transmitted at a rate that grossly exceeds the computing power of desktop computers.  These are practical challenges that need to be addressed in order to maximize the potential of these new data.   

Another feature that distinguishes data science from traditional academic research is the focus on generating _actionable insights_ from data rather than theory building and hypothesis testing. In fact, data science is more interested in deriving useful insights from correlational patterns as opposed to uncovering causal mechanisms.  The focus is on the development of data products (Davenport, XXX), as opposed to reporting analysis.  Data products are tools or services that generate actionable insights from the data itself, including (but not limited to) automated reports or dashboards, recommendation systems, prediction algorithms, decision tools, and interactive visualizations that help people with non-technical background make sense of complex data.  Data science also values the philosophy of _openness_ or open science.  Thus, much effort is devoted to creating sustainable systems that make data as widely and freely accessible as possible, and developing and promoting the use of open source tools for managing and analyzing data.  The concept of reproducible research is a _rule_ rather than the _exception_.  This involves making both data and statistical code available alongside research reports.  This helps ensure the quality of the research, in addition to allowing researchers to make advances without having duplicate work that has already been done.  Data science is by no means incompatible with traditional academic research.  We believe that the tools, research strategies, and principles of data science are complementary to traditional ways of knowledge building in social work, offering many unique opportunities to improve the quality, efficiency, and impact of this area of research.  

### Overview of Current Study 

In the current study, we seek to utilize the opportunities of the data revolution and data science by applying a range of tools and strategies to construct an historical base of social work research.  In doing so we take on two of the 3 Vs of big data: volume due the amount of data collected and variety in terms of the multiple sources of that data.  The data are article records harvested from existing bibliographic databases hosted on ProQuest and EbscoHost.  Article records are already widely used in various bibliometric studies, particularly those that are intended to reveal publication networks within scientific communities.  It does so by creating links between co-authors, and then visually representing the edges and vertices of the network.  This is also done with article citation histories, to show influence over time.  In this study, we were interested in the various article meta-data that are common to many scientific article records.  For example, a single article record in psycINFO can have as many as XX pieces of meta-data including:  dates, journal titles, article titles, author names, author affiliations, full text abstracts, keywords, subject classification, methodology, location, populations, page numbers, and source of funding.  This is a rather large amount of information about a single paper, at least compared to the amount of information contained in article records before the availability of electronic storage and access.  Large collections of article records may contain potentially interesting trends in the research, assuming the selection of the article records were conducted in a way that allowed meaningful inferences to be derived. 

In the current study, we sought to capture every journal article record published in every social work journals over the in past quarter century (1989-2013) and to explore the various meta-data to uncover potentially interesting trends in the social work scholarship.  For example, we can reasonably infer estimates of the _size_  of social work scholarship by counting the number of unique article records.  And, growth of the field can be inferred by examining the number of article records over time.  Similarly, we examine the extent to which _team science_ has become part of the research practice, as this has implications for training of social work researchers.  We also use location meta-data to understand the extent to which different areas of the world have and have not been the focus of social work research.  Finally, we used topic modeling to extract topic areas that define the focus of social work research.  The execution of this research was grounded in the tools and values of data science.  Thus, all data were managed and analyzed using the open source software, R.  We also used an open source authoring system that integrates the statistical analysis with the text of the manuscript.  Along with this manuscript, the authors will also release the actual data and statistical code, thereby meeting the requirements of _reproducible research_, as defined by King ().  To our knowledge, this is the first time fully reproducible research (as defined by King, XXXX), has been published in a social work journal.    

Davenport: http://blogs.wsj.com/cio/2014/06/25/so-you-want-to-build-a-data-product/

http://www.nsf.gov/pubs/2015/nsf15544/nsf15544.htm

http://www.nsf.gov/news/news_summ.jsp?cntn_id=123607&org=NSF&from=news


Why is this study important:

1.  Substantive contributions 
Understand publication trends from a macro perspective.  This can:

From an economic standpoint, the field of social work observed growth
throughout the economic crisis and is expected to maintain growth.  The number
of MSW programs also continues to grow.  While growth is observed in a variety
of areas, it is important to consider the extent to which growth has occurred
regarding overall scholarship.  

From a methodological standpoint, this study makes a few unique and important
contributions to the social work research.  Foremost, the study was designed
and executed in a manner that meets the full standards of reproducible
research.  The CTSPEDIA, an advocate of fully reproducible research, defines
this type of research as, ``the practice of distributing, along with a research
publication, all data, software source code, and tools required to reproduce
the results discussed in the publication.  As such the RR [reproducible
research] package not only describes the research and its results, but becomes
a complete laboratory in which the research can be reproduced and extended."
(NOTE ABOUT ORGMODE)

Methods

Include section on reproducible research.  


Discussion

## Growth of Social Work Research

From the years 1989 to 2013, the field of social work research produced nearly
35,000 journal articles. This doesn't include books or book chapters,
editorials, book reviews, and other types scientific communications.  The
growth increased steadily from XX to XX, and appears to be leveling off.
Through this period of time, we observed XX unique titles appear while a number
of other journals were lost. 

The current study helps clarify the extent to which _team science_ is emerging
in social work research.  This is an important issue to consider, as it has
implications for training researchers, reviewing and understnanding tenure and
promotion cases, and research infrastructure to facilitate collaborations
(http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3321548/). According to Thomson
Reuters, the average number of authors on papers included in the Science
Citation Index increased by about 50% between 1990 and 2010
(http://www.hopkinsmedicine.org/institute_basic_biomedical_sciences/news_events/articles_and_stories/employment/2013_01_middle_authors.html)
In our research, a t least one-half of all published studies were authored by a
single individual until 2009.  The standard deviation continues to increase,
providing further evidence of continued team science growth in social work.
How authorship is assessed is a very important consideration for the field,
particularly with respect to tenure reviews.  It is not clear how reviewers of
tenure cases are viewing team science contributions and _middle authors_.
However, this is something that will have to be considered for the field. In
the field of physics, the weighted average seniourity and average seniority
measures both had a negative relationship with the impact, based on the number
of citations the publication received, as well as a negative relationship
between the number of authors and the number of citations.  http://myweb.fsu.edu/bstvilia/papers/AuthorTeamDiversityImpact.pdf
(***NOTE TO BRYAN -- AS A QUICK STUDY, YOU AND I SHOULD BUST OUT A STUDY
INVOLVING THIS.  WE CAN DO IT FOR JUST THE TOP JOURNALS IN SOCIAL WORK THAT ARE
IN WOS.)

http://dmm.biologists.org/content/7/3/311.full

https://ccrod.cancer.gov/confluence/download/attachments/47284665/teamscience_fieldguide.pdf
    

One thing to consider is the extent to which social work research is being
served by other journal outlets.  For example, social work researchers with
primary research interests in areas of health, mental health, and substance use
may publish in speciality journals outside of social work for a variety of
reasons.  Thus, the amount that social work has grown should be cautious,
keeping in mind this particular metric refers only to social work journals.
The field may have grown more rapidly in other areas that are not reflected in
the current research.  The functions that were constructed to _wrangle_ the
abstracts are functional for records extracted from PubMed.  Since this data
source contains author affiliations, it would be feasible to extract this
information using a set of regular expressions. 

According to the journal Nature, from the lat 1600s until about 1920, the rule
was one author per paper.  The rules was ``breached in the 1920s, diminished in
the 1950s, and largely abandoned by the 1980s'' (p.
http://www.nature.com/nature/history/full/nature06243.html).  Social work
research had a much different trajectory.  In social work research, the rule
wasn't breached until 2009.  

This has implications for tenure, and it is unclear how _team science_ is
actually being considered in context of these reviews.  The field is arguably
in a time of transition, form si

Defining a _disciplinary_ social work journal was a complicated task.  Social
work is inherently inter-disciplinary.  We based our definition of a social
work journal using the list of Hodge and Lacasse (2011), who created a
comprehensive list of journal titles.  This list was based on earlier attempts
to identify all the disciplinary social work journals.  We supplemented our
list by doing supplemental searches in all data sources for journals that used
any of the following terms.  


The length of the articles was examined, as it has implications for device
formatting.  Clearly, the field of social work has a current average of 15
typeset pages.  This consistency allows for targeted customization for mobile
devices.  An article can be optimized and fairly standardized, which has
important benefits for information retrieval and comprehension.  

This article raised many opportunities that became obvious in this study.  In
the current study, we worked in earnest to present as much of the analyses
visually.  At the time of writing this article, the majority of social work
journals do not have full color articles.  Indeed, this is a residual policy
from when journals were exclusively in black-and-white and before electronic
archiving.  Now, the majority of research is disseminated electronically, and
even the most primitive computers connected to the Internet have color
monitors.  

Data quality -- is there anything to talk about on this topic???

Team science -- is it good for social work research?  


## Reproducible research

Reproducible research is essential to ensuring the integrity of the scientific
record.  Making research reproducible is a complicated process that does carry
risks.  More specifically, it does involve taking extra time to prepare
documentation, organize code, and prepare files to be made publicly available.
This problem has already been given careful attention in other fields, which
can be considered reasonable solutions for solution work research.  For
example, this study utilized Rmarkdown as the tool for producing reproducible
research.  Rmarkdown is ... [provide *short* description here -- tell reader
minimum she/he needs to know].  

Another practical consideration of reproducible research is curating the data,
and making it available.  The availability can be challenging.  For example, 
many social work studies involve sensitive data that ultimately requires very
strict limitations on how and where the data are stored, along with
restrictions on who can access the data.  [Describe the PS journal that helps
address this issue ...]  This study involves proprietary data from EbscoHost
and ProQuest, which prevents us from making all the data available.  However,
we provide a minimum random extract of the data to ensure verification of our
computer code.  Additionally, we have made all aspects of our computer code
available that will allow researchers with access to EbscoHost and ProQuest to
replicate our findings, as well as use the code to build upon this research. 

We believe that social work journals should consider implementing policies that
encourage reproducible research.  Making this best practice in publishing
represents a straightforward way to improve the quality of social work
research.  Along with a policy of reproducible research, the issue of how the
journal responds to errors will also be necessary.  We certainly do not
advocate that authors are expected to retract articles or face public shame in
the context of random human error.  Some studies may involve generating
thousands of lines of computer code -- and, a simple bug in the code can
produce erroneous results.  Researchers who adhere to the principles of
reproducible research are ultimately working in earnest to ensure they are
producing the highest quality research possible.  Errors that are identified in
the process represent an opportunity to further advance knowledge, which
sometimes means correcting knowledge.  These situations simply reveal that the
scientific method is actually working the way it is supposed to.  At present,
we have to assume that all the social work research published to date is
without error.  In absence of reproducible research, we can not fully assess
the quality of research or replicate a study.   
  

Discussion point:  

Another complication is the possible consequences that a researcher might experience
when an error is discovered.  This is an important issue that the field needs
to address.  



very research report is subject to some form of human
error.  It is important to consider some of the different types of error and
how the field should respond.  


(Conduct and analysis on reproducible research" and "replication" or
"replicate")  


(original citation from orgmode.org:
http://orgmode.org/worg/org-contrib/babel/intro.html)   


 conforms to all
aspects of what is regarded as reproducible research by making available all
the code produced in order to conduct the study.

.  Methodological contributions


# Methods

### Extraction of article records

Our list of social work journals was guided by the work of Hodge and Lacasse
(2011).  More specifically, Hodge and Lacasse (2011) identified 84 disciplinary
social work journals based on a variety of sources, including _An Author’s
Guide to Social Work Journals_ (NASW Press, 1997), Thyer’s (2005) more recent
listing of social work periodicals, and Genamics JournalSeek
(http://journalseek.net/). Hodge and Lacasse (2011) examined the mission and
aims of each journal, and eliminated journals that were specific to another
field or had an had an inter-disciplinary focus.  For the current study, our
primary search query included all journal titles in this list.

To help ensure comprehensive coverage of all possible core social work
journals, we created a supplemental search query that extracted journal titles
with any of the following terms:  

+ "social work"
+ "social welfare"
+ "social casework"
+ "social service" 
+ "human  service" 
+ "social development" 
+ "social environment"

Using this search queries, we extract journal article recrods from three major
databases on the EbscoHost platform:  PsycINFO, Social Service Abstracts, and
Social Work Abstracts.  Because of known gaps in indexing in databases (Holden,
Barker, Kuppens, Rosenberg & LeBreton, 2014; Holden, Barker, Covert-Vail,
Rosenberg, & Cohen, 2009), we also searched ProQuest, which linked to an
additional 46 minor databases, including (but not limited to): ERIC,
Sociological Abstracts and Worldwide Political Science Abstracts.  In every
search, we used a filter in attempt to extract only article records that were
classified as _journal articles_, thereby excluding other forms of scholarly
communication that was not relevant to the current study (e.g., book reviews,
editorials, obituaries, etc.).  We limited the timeframe to a 25 year period,
from 1989 to 2013 (inclusive).  Articles published in years 2014 and 2015 were
excluded because of delays in indexing.  Search results were exported in
batches of article records based on the restrictions of the platform.  Article
records were text files in a _generic bibliographic format_.  The article
records contained various meta-data based on the database from which the
article record was extracted.   Meta-data include (but are not limited to):
article title, journal title, publication year, author name(s), author
affiliation(s), abstract, keywords, methodological classification, funding
source, location of study, subject groups, digital object identifier (DOI),
number of references, number of pages, etc.  These files were post-processed
into a structured database using a set of scripts written in the R statistical
programming language.  The initial search resulted in `r
pretty(nrow(article.count.initial))` article records from `r
journal.count.initial`.

The search queries were purposefully specified to be overly inclusive to ensure
full coverage of all possible social work journals contained in the major and
minor databases.  Additionally, a visual inspection of the article records
revealed problems in the original indexing of articles that requires subsequent
data cleaning.  Thus, our data cleaning procedures focused separately on
journal titles and article records.

### Data cleaning: Journal titles

The first step of data cleaning involved fixing journal title names due to
discrepant errors in indexing. For example, _Journal of Gay and Lesbian Social
Services_ was indexed as a journal separate from _Journal of Gay & Lesbian
Social Services_ (use of "and" vs. "&").  Other examples involved journals
indexed with and without subtitles, or journals with and without the word "The"
at the beginning of the title.  Additionally, some journals changed titles over
their history.  For example, _Journal of Technnology in Human Services_ is
formerly known as _Computers in Human Services_.  In these situations, the
former titles were merged with the current titles.  

Many journals from allied health disciplines used one of the supplemental
search terms in special editions, which were also included in the journal
title.  Thus, many non-social work journals were captured in the extraction
process.  To resolve this issue, we created a list of all journal titles that
were not part of the core list defined by Hodge and Lacasse (2011).  Study
authors reviewed these journal titles and discussed whether each candidate
title should be retained or excluded.  When disagreements occurred, the study
authors reviewed the mission and aims of the journals, names of editorial board
members, and focus of the articles.  A consensus was reached on all journal
titles to be excluded and retained.   After these procedures, the number of
article records and journal titles was reduced to `r
pretty(nrow(article.count.journalsCleaned))` and `r
journal.count.journalsCleaned`.  

### Data cleaning:  Article records

Our search procedures involved the use of three major databased and 46 minor
databases, which resulted in some duplication of article records.  Thus, a
matching algorithm was constructed to identify and remove all duplicate article
records. As previously noted, the filtering of only journal articles was not
successful due to errors in the original indexing.  Thus, other scholarly
communications were captured in the extractio process.  These other scholarly
communications were readily identified due to specific patterns in the titles,
such as the terms "Book Review," "From the Editor," and "Obituary."  Given the
number of article records in the database, it was not feasbile to manually
extract these recrods.  Instead, we created a separate database with
representative examples of article records that we wanted to extract.  We also
added representative examples of article records to be retained.  We then wrote
a series of _regular expressions_ to extract the problematic article records.
Regular expressions are a sequence of characters that form search patterns,
allowing extraction of records that match the specified pattern.  In other
words, it is a more sophisticated implementation of the search function common
to all major word processors.  We tested our regular expressions on the test
database, achieving > 95% accuracy with regard to retaining and extracting
article records.  These regular expressions were then applied to the full
database.  We then excluded article records if they were missing essential
meta-data, including author name(s), journal title, article title, and
publication field.  As an additional requirement, we required all articles to
be $\geq 3$ pages in length.   Finally, we eliminated all social work journals
with total article counts of < 10 for the entire 25 year window (_Issues in
Social Work Education_, _Maatskaplike Werk/Social Work_, _Pediatric Social
Work_, and _Critical Social Work_).  These cleaning procedures resulted in a
final database of `r pretty(articles.final)` articles and `r journals.final`
journals.  

### Data quality checks

We performed a variety of checks on the final database.  Because the size of
the database, it was not feasible to manually inspect all records.  Thus, we
randomly sampled article records from the database and cross-checked the data
we procured with the information listed with the journal's homepage.  We also
inspected a scatter plot of the number of article records for each journal by
the number of years the journal appeared within the 25 window timeframe.
Journals that exhibited significant deviation from the regression line were
inspected to ensure that our cleaning procedures did not systematically exclude
article records or retain other scholarly communications.  All observed
discrepancies were verified as differences in journals actual publication
output.  Finally, we also made checks of number of journal article records for
various journals indexed in the _Web of Science_.  Although the counts were
close -- i.e., within 10% -- similar indexing problems observed from our record
extraction were also observed in the _Web of Science_ database, such as
misclassification of book reviews as journal articles.  Thus, we are unable to
quantify the actual amount of error in our database is due to problems in
indexing.  At the same time, we are relying on the same source of data that
social work researchers use to inform their work.  Too much cleaning and
post-processing of the search results can give rise to validity issues because
social work researchers are using an information source that contain the errors
we have attempted to eliminate.  The issue of reliability and validity are
given further attention in the discussion section of this study.  

### Analyses

# Results
```{r}
# As shown in Table 1, `r nrow(final.list.25)` journals were included in the current study.  The mean number of years of inclusion is `r options(digits=2); mean(final.list.updated$study.time)` (median = `r options(digits=2); median(final.list.updated$study.time);`, sd =  `r options(digits=2); sd(final.list.updated$study.time)`). The overall h-Index for each journal.  The h-Index values represent the current h-Index of each journal, as opposed to the h-Index of the final study period, so these values should be interpreted cautiously.  The mean h-Index value is XXXX, (median = XXX; sd = XXX). A total of `r pretty(sum(articles.year.df$year.count))` article records were obtained across these journals.  The average number of articles per journal was `r options(digits=2); pretty(mean(articles.year.df$year.count))` (median = `r options(digits=2); pretty(median(articles.year.df$year.count))`, sd = `r options(digits=2); pretty(sd(articles.year.df$year.count))`). This figure shows consistent growth in the number of articles from 1989 to 2005.  Notable fluctuations in the number of articles was observed from 2005 to 2008, followed by growth until 2012. The decline from 2012 to 2013 should be interpreted cautiously, as this may be an artifact due to delays in indexing.  
```

### Aggregated summary (Figure 1a. 1b. -- line charts) 

Figures 1a and 1b are line charts showing the number of unique journal
titles and journal articles published each year during the study time frame.
In 1989, XX unique journals published a XX articles.  The number of journal
titles showed some variability from 1989 to 1996, followed by nine years of
steady growth.  By 2005, the number of journal titles and journal articles
being published was more than double than what was observed in 1989. The years
2005 to 2013 exhibited some variability in the number of titles and articles
published.  The maximum number of journal titles was observed in 2011 (n = XX)
and dropped to XX by 2013.  The maximum number of journal articles published
was in 2012, and this dropped to XX in 2013.  The cumulative number of journal
articles published over the study time frame was XX,XXX.     





### Dis-aggregated summary (Figure 2. Small multiples)
 
Figure 2 is a small multiples plot, also referred to as a trellis or lattice
chart, that dis-aggregates Figures 1a and 1b.  More specifically, the small
multiples plot shows the number of articles published each year for each 
journal throughout the study time frame (orange line).  We also included data from Scopus for
comparative purposes (purple line).  The small multiples are sorted in descending
order based on the total number of article records in the HD.  The presentation
of the sort is by row, so the overall plot is read from left to right.
Abbreviations are used in Figure 2, and the full journal name is presented in
Appendix A.   

The small multiples plot is particularly useful to show the variability in
publishing for each journal.  In fact, according to Tufte (xxxx), small
multiples are the best design solution for visually enforcing comparisons of
change.  The plot shows that _Families in Society_, _Social Work_, and _British
Journal of Social Work_ published the greatest number of articles throughout
the study time frame.  However, the number of articles published by _Families
in Society_ and _Social Work_ appear to be decreasing, while other journals
show an increase -- e.g., _Research on Social Work Practice_, _International
Social Work_, and _Journal of Human Behavior and the Social Environment_.   

Data from Scopus was used to help show the overall coverage of the HD, in
addition to revealing possible gaps in indexing.  The HD and Scopus plotted
values do not show exact alignment for a couple reasons.  More specifically,
the HD contains article records going back to 1989, whereas Scopus provides
data starting from 1996.  Additionally, the overall journal coverage of HD is
much greater than Scopus -- that is, Scopus provides data for only xx% of the
journal titles contained in the HD.  

The Scopus data that are available show a high degree of correspondence with
the HD.  Small discrepancies between the Scopus and HD can be attributed to
differences in indexing.  That is, the article record count of Scopus includes
editorials and book reviews, whereas these were excluded from the HD.  In the
construction of the HD, we applied a series of cleaning procedures (refer to
Methods section) that further reduced the article count.  Because Scopus data
were available only in aggregate form, it was not possible to extract these
data to make more refined comparisons.  

And, finally, the small multiples plot is especially useful for identifying
potential gaps in indexing, which is a problem that has been previously
identified with respect to social work journals (see Holden; Holden). For
example, Scopus appears to be missing article records for the _Journal of
Gerontological Social Work_ from XX to XX.  And, the HD is missing records for
various journals including (but not limited to) the _Journal of Ethnic and
Cultural Diversity in Social Work_, _Journal of Social Work Disability &
Rehabilitation_.


### Article attributes

Figures 3a and 3b provide summaries of two important article attributes -- that
is, the number of authors per article and the length of each article.
Throughout the history of social work research, at least half of all social
work articles were sole authored until 2008, as indicated by the median value
in Figure 3a.  Since 2008, at least half of all articles had two authors.
Throughout the study time frame, the number of authors has increased steadily,
as shown by the increasing mean value and its standard deviation.  In 2013, the
mean number of authors was approximately 2.25, with a standard deviation of
1.4.  The length of the articles increased steadily from approximately 12 pages
in 1989 to 15 pages in 2004.  The last three years of the study time frame
shows a fairly stable publishing trend, with half of all articles being 15
pages (Mean = XX, sd = XX).   

### Abstract analysis

The final set of analyses involved text analysis of the article abstracts to
determine whether trends in publishing could be readily identified based on the
frequencies of specific words contained in article abstracts.  In this analysis, we selected words that
represented different _levels of evidence_ within the evidence-based practices
framework.  EBP was selected as the focus of analysis, given that the topic
area is a relatively new in social work, compared to other topics like child
welfare and case management.  Moreover, because EBP has received considerable
attention in various areas of social work research, it was expected that
the different _levels of evidence_ would show should both absolute and
relative changes in their occurrences. 

In this analysis, we focused on four different _levels of evidence_:
Meta-analysis, systematic reviews, randomized controlled trials (RCTs), and
quasi-experimental designs.  To identify these key words in the article
abstracts, we created a set of regular expressions of common spellings and
synonyms for each type of evidence (see Appendix B).  Regular expressions, also
referred to as regex and regexp, are sequences of characters that define
specific patterns in text documents and work much like the _find_ functions
common to word processors.  The regular expressions used to define the _levels
of evidence_ are presented in Appendix B and the study's source code.   

A stacked line chart was used to show absolute change in the number of
occurrences for each level of evidence in article abstracts.  The line chart
shows increased numbers of occurrences for each level of evidence, with the
greatest amount of growth starting in year 2008.  The presence of systematic
reviews and meta-analyses did not have a sustained presence in  the literature
until approximately 1995.  In fact, the first occurrence of the key word
_systematic review_ did not appear until 1997 (cite). 

To show relative change, we plotted the proportion of overall articles
published that contained the target key words for each year.  In 1989, RCTs and
Quasi-experimental studies were observed in approximately 1/2% of published
articles.  By 2013, the percentage of RCTs tripled and the percentage of
quasi-experimental doubled.  Meta-analyses appeared in 1989 and grew to almost
1/2% by 2013.  Systematic reviews first appeared in 1997, then again in 2003,
and grew to approximately 1% in 2013.   

**Pareto chart**



\newpage

```{r finalArticleListecho=FALSE, comment=NA}
#print.data.frame(final.list, row.names=FALSE)
```

\newpage

```{r}
#plotIndex
```

\newpage

```{r}
grid.arrange(plot.journal.count, plot.article.count, ncol=2)

```
\newpage
```{r fig.height=10, fig.width=7.5}
plotSmallMultiples.1

```

\newpage
```{r fig.height=10, fig.width=7.5}
plotSmallMultiples.2

```


\newpage

```{r}
#plot.article.count
```


\newpage

```{r}
plot.author.count
```

\newpage

```{r}
page.plot
```

\newpage

```{r}
evidence.plot
```
\newpage

```{r}
stacked.plot
```
\newpage
```{r}
journal.evidence.plot
```

\newpage
```{r}
EBP.plot
```


\newpage

# Appendix A

Articles on Hodge and Lacasse (2011) but not in the current study.  

```{r appendix_Print, eval=TRUE}
print(sort(hodge.swHistory.diff))
print(fewArticles)
```
