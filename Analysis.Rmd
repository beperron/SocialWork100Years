---
title: '100 Years of Social Work Research: A Data Science Perspective'
output: pdf_document
---

# Overview of data

The original data were from a search of PsychInfo using Ebsco Host platform (December 23, 2014). The following search operators and limiters were used:   

+ SO "social work" OR SO "social welfare" OR SO "social casework" OR SO "social services"    

+ Limiters - Document Type: Journal Article 

+ Search modes - Boolean/Phrase	Interface - EBSCOhost Research Databases 

+ Search Screen - Advanced Search 

+ Database - PsycINFO

The search results were exported in a _generic bibliographic format_, which is an unstructured text (*.txt) file.The text file was processed using the `BibWrangleR` function created by the first author.  


# Initialize workspace and functions for data wrangling

This section processes raw data.  This section of code is executed only one time to transform raw text data into an analyzable format.  When new data are obtained for this study (i.e., updated search results), this section should be re-run by changing `echo=FALSE` to `echo=TRUE` in the knitr markdown argument.   

```{r initialization_dataprocessing, warning=FALSE, message=FALSE, eval=FALSE}
# Clear workspace
rm(list=ls())

# Read BWR functions for Mac OS
source("/Users/beperron/Git/BibWrangleR/functions/piWrangleR.R")
source("/Users/beperron/Git/BibWrangleR/functions/packages.R")

# Set the path where original raw data are stored
setwd("/Users/beperron/Git/SocialWorkResearch")

# Set the working directory to store files created by BWR functions
my.path <- "/Users/beperron/Git/SocialWorkResearch"

# Wrangle the data with the BWR function suite
#piBWR.f(csv=FALSE, path=my.path)
#save(pi.df, file = "piArticles.R")
```

# Initialize workspace and functions for analaysis 

All the analyses performed involve the data that have been processed with the `BibWrangleR` functions.  This section reads the processed data, loads the required packages, and does a quick quality check to ensure that the same number of articles (i.e., records) contained in the original search match the number of articles in the transformed data.  

```{r initialization_analysis, message=FALSE, warning=FALSE, comment=NA}
rm(list=ls())
load("piArticles.R")
library(dplyr)
library(ggplot2)
library(gridExtra)

# Inspect dimensions of the data file (Rows X Columns)
dim(pi.df)

# Inspect variable names of the data file
names(pi.df)

# How many unique article titles?  Ebsco Results of most current search is $n=24,314$. Do not proceed with analysis if the output does not match this result.  
length(which(pi.df$attributes == "TI"))
```


## What is the overall number and names of journal titles? 

```{r unique_titles}
unique.titles <- filter(pi.df, attributes == "SO")

# Number of unique titles
length(unique(unique.titles$record))

# Unique titles
unique(unique.titles$record)
```

## Number of unique journal titles by year

```{r unique_titles_year, message=FALSE, comment=NA, warning=FALSE, fig.height=2, fig.height=4}
journals.year <- tbl_df(pi.df)

year <- journals.year %>%
        filter(attributes == "YR") %>%
        select(id = articleID, year = record)

journals <- journals.year %>%
        filter(attributes == "SO") %>%
        select(id = articleID, journal.title = record)

n.journals.year <- journals %>% 
        left_join(year) %>%
        group_by(year) %>%
        distinct(journal.title) %>%
        summarise(n = n())

journal.count <- ggplot(n.journals.year, aes(as.numeric(year), y=n, group=1)) + 
    geom_line(colour="black") +
    #geom_point(colour="red") + 
    theme(axis.text.x = element_text(angle = 45, hjust = 1)) + 
    xlab("year") + 
    ylab("frequency") + 
    ggtitle("Number of Journals by Year") + 
    scale_x_continuous(breaks=seq(1914, 2014, 5))
        
journal.count
```


## What journals published the most number of articles

```{r journal_output}

n.so.yr <- filter(pi.df, attributes == "SO" | attributes == "YR")

n.so <- filter(pi.df, attributes == "SO") %>% mutate(title = record) %>% 
        select(-attributes, -record)

n.yr <- filter(pi.df, attributes == "YR") %>% mutate(year = record ) %>% 
        select(-attributes, -record)

n.so.yr <- left_join(n.so, n.yr) %>%
    group_by(title) %>%
    summarise(first = min(year), last = max(year), n.to.date = n()) %>%
    arrange(desc(n.to.date))

# 10 highest number of publications
head(n.so.yr, 10)
```

## What is the lifespan of journals?  

```{r journal_lifespan}
#10 longest running journals 
longest.running <- n.so.yr %>%
       mutate(last = as.numeric(last), first = as.numeric(first), 
              year.diff = last - first) %>%
       arrange(desc(year.diff)) %>%
       select(title, first, last, year.diff)

head(longest.running, 10)

#10 shortest running journals
shortest.running <- longest.running %>% arrange(year.diff, first, last)
head(shortest.running, 20)
```

## What is the number of articles published per year

```{r articles_per_year}
n.articles.year <- filter(pi.df, attributes == "YR") 
year.split <- split(n.articles.year, n.articles.year$record)
year.count <- unlist(lapply(year.split, nrow))
year.count <- year.count[order(names(year.count))]
years <- names(year.count)

df <- data.frame(years, year.count)
rownames(df) <- NULL

plot.article.count <- ggplot(df, aes(as.factor(years), 
                    y = year.count, group=1)) + 
    geom_line(colour="black") +
    #geom_point(colour="red") + 
    theme(axis.text.x = element_text(angle = 45, hjust = 1)) + 
    xlab("year") + 
    ylab("count") + 
    ggtitle("Number of Studies by Year") +
    scale_x_discrete(breaks=c(seq(1914, 2014, 10))) +
    scale_y_continuous(breaks = c(seq(0, 2000, 250))) 

df$years <- as.numeric(as.character(df$years))

plot.article.cumulative <- ggplot(df, aes(x = years, y = cumsum(year.count))) + 
    geom_line() + 
    theme(axis.text.x = element_text(angle=45, hjust=1)) +
    scale_x_continuous(breaks=pretty(df$years)) +
    xlab("year") +  
    ylab("count") +
    scale_x_continuous(breaks = c(seq(1914,2014,10))) +
    scale_y_continuous(breaks = c(seq(0, 25000, 2500))) + 
    ggtitle("Cumulative Frequency")

grid.arrange(plot.article.count, plot.article.cumulative, ncol=2)

# Print most recent ten years
head(df, 10)
```


# What are the topic areas (by Subject Terms)?

```{r subject_terms_overall, comment=NA, warning=FALSE, message=FALSE}

su.df <- filter(pi.df, attributes == "SU")

subject.terms <- stringr::str_split(su.df$record, pattern = ";")
subject.terms <- unlist(lapply(subject.terms, function(x) gsub(" ", "", x)))

subject.terms.total <- length(unlist(lapply(subject.terms, 
                        function(x) gsub(" ", "", x))))

subject.terms.unique <- length(unique(subject.terms))

subject.terms.l <- list(subject.terms.total = subject.terms.total,
                      subject.terms.unique = subject.terms.unique)

most.frequent <- as.data.frame(table(subject.terms))

most.frequent <- arrange(most.frequent, desc(Freq))

# Print summary statistics in list form
subject.terms.l

# Print 25 most commmon terms
head(most.frequent, 25)

```

## What are the topic areas over time (by Subject terms)?

```{r subject_terms_overtime}
decade <- filter(pi.df, attributes == "YR") %>%
    mutate(year = as.numeric(record)) %>% select(-record, -attributes)

decade$year <- cut(decade$year, breaks = 20, labels = c(1:20))

keywords <- pi.df %>%
        filter(attributes == "SU") %>%
        select(articleID = articleID, keywords = record)

keywords.decade <- keywords %>% 
         left_join(decade)

library(plyr)
keywords.data.split <- dlply(keywords.decade, .(year))
detach(package:plyr)

terms.f <- function(x){
    split.terms <- stringr::str_split(x[,"keywords"], pattern =";")
    clean.terms <- lapply(split.terms, function(x) gsub(" ", "", x))
    }
    
keywords.decade <- lapply(keywords.data.split, terms.f)
keywords.decade <- lapply(keywords.decade, unlist)
lapply(keywords.decade, function(x) length(unique(x)))

temp <- lapply(keywords.decade, function(x) data.frame(table(x)))
temp <- lapply(temp, function(x) arrange(x, desc(Freq)))
lapply(temp, function(x) head(x,10))    
```


# What are topic areas over time (by author specified keywords)?
  
```{r comment=NA, warning=FALSE, message=FALSE}
kp.df <- filter(pi.df, attributes == "KP")

subject.terms <- stringr::str_split(kp.df$record, pattern = ";")
subject.terms <- unlist(lapply(subject.terms, function(x) gsub(" ", "", x)))
subject.terms.total <- length(unlist(lapply(subject.terms, 
                        function(x) gsub(" ", "", x))))

subject.terms.unique <- length(unique(subject.terms))

subject.terms.l <- list(subject.terms.total = subject.terms.total,
                      subject.terms.unique = subject.terms.unique)

most.frequent <- as.data.frame(table(subject.terms))

most.frequent <- arrange(most.frequent, desc(Freq))

# Print summary statistics
print(subject.terms.l)

# Print 25 most frequent
head(most.frequent, 25)
```

## Most Frequent Author Keywords

```{r}
decade <- filter(pi.df, attributes == "YR") %>%
    mutate(year = as.numeric(record)) %>% select(-record, -attributes)

decade$year <- cut(decade$year, breaks = 20, labels = c(1:20))

keywords <- pi.df %>%
        filter(attributes == "KP") %>%
        select(articleID = articleID, keywords = record)

keywords.decade <- keywords %>% 
         left_join(decade)

library(plyr)
keywords.data.split <- dlply(keywords.decade, .(year))
detach(package:plyr)

terms.f <- function(x){
    split.terms <- stringr::str_split(x[,"keywords"], pattern =";")
    clean.terms <- lapply(split.terms, function(x) gsub(" ", "", x))
    }
    
keywords.decade <- lapply(keywords.data.split, terms.f)
keywords.decade <- lapply(keywords.decade, unlist)
lapply(keywords.decade, function(x) length(unique(x)))

temp <- lapply(keywords.decade, function(x) data.frame(table(x)))
temp <- lapply(temp, function(x) arrange(x, desc(Freq)))
lapply(temp, function(x) head(x,10))    
```



# Location

It is easy to explore some of the different fields within the PsychInfo data frame.  For example, each record has one or more subject terms (from the article keywords).  The total number, unique number, and most frequently occuring key words can be easily computed.  

```{r comment=NA, warning=FALSE, message=FALSE}

LO.df <- filter(pi.df, attributes == "LO")


subject.terms <- stringr::str_split(LO.df$record, pattern = ";")
subject.terms <- unlist(lapply(subject.terms, function(x) gsub(" ", "", x)))
subject.terms.total <- length(unlist(lapply(subject.terms, function(x) gsub(" ", "", x))))
subject.terms.unique <- length(unique(subject.terms))

subject.terms.l <- list(subject.terms.total = subject.terms.total,
                      subject.terms.unique = subject.terms.unique)

most.frequent <- as.data.frame(table(subject.terms))



most.frequent <- arrange(most.frequent, desc(Freq))
most.frequent.t <- head(most.frequent, 50)

print(subject.terms.l)
print(most.frequent.t)

most.frequent

```

## Location over time

```{r}
decade <- filter(pi.df, attributes == "YR") %>%
    mutate(year = as.numeric(record)) %>% select(-record, -attributes)

decade$year <- cut(decade$year, breaks = 20, labels = c(1:20))

keywords <- pi.df %>%
        filter(attributes == "LO") %>%
        select(articleID = articleID, keywords = record)

keywords.decade <- keywords %>% 
         left_join(decade)

library(plyr)
keywords.data.split <- dlply(keywords.decade, .(year))
detach(package:plyr)

terms.f <- function(x){
    split.terms <- stringr::str_split(x[,"keywords"], pattern =";")
    clean.terms <- lapply(split.terms, function(x) gsub(" ", "", x))
    }
    
keywords.decade <- lapply(keywords.data.split, terms.f)
keywords.decade <- lapply(keywords.decade, unlist)
lapply(keywords.decade, function(x) length(unique(x)))

temp <- lapply(keywords.decade, function(x) data.frame(table(x)))
temp <- lapply(temp, function(x) arrange(x, desc(Freq)))
lapply(temp, function(x) head(x,10))    
```





# Methodology

It is easy to explore some of the different fields within the PsychInfo data frame.  For example, each record has one or more subject terms (from the article keywords).  The total number, unique number, and most frequently occuring key words can be easily computed.  

```{r comment=NA, warning=FALSE, message=FALSE}

MD.df <- filter(pi.df, attributes == "MD")


subject.terms <- stringr::str_split(MD.df$record, pattern = ";")
subject.terms <- unlist(lapply(subject.terms, function(x) gsub(" ", "", x)))
subject.terms.total <- length(unlist(lapply(subject.terms, function(x) gsub(" ", "", x))))
subject.terms.unique <- length(unique(subject.terms))

subject.terms.l <- list(subject.terms.total = subject.terms.total,
                      subject.terms.unique = subject.terms.unique)

most.frequent <- as.data.frame(table(subject.terms))



most.frequent <- arrange(most.frequent, desc(Freq))
most.frequent.t <- head(most.frequent, 50)

print(subject.terms.l)
print(most.frequent.t)

```

## Methodology

```{r}
decade <- filter(pi.df, attributes == "YR") %>%
    mutate(year = as.numeric(record)) %>% select(-record, -attributes)

decade$year <- cut(decade$year, breaks = 20, labels = c(1:20))

keywords <- pi.df %>%
        filter(attributes == "MD") %>%
        select(articleID = articleID, keywords = record)

keywords.decade <- keywords %>% 
         left_join(decade)

library(plyr)
keywords.data.split <- dlply(keywords.decade, .(year))
detach(package:plyr)

terms.f <- function(x){
    split.terms <- stringr::str_split(x[,"keywords"], pattern =";")
    clean.terms <- lapply(split.terms, function(x) gsub(" ", "", x))
    }
    
keywords.decade <- lapply(keywords.data.split, terms.f)
keywords.decade <- lapply(keywords.decade, unlist)
lapply(keywords.decade, function(x) length(unique(x)))

temp <- lapply(keywords.decade, function(x) data.frame(table(x)))
temp <- lapply(temp, function(x) arrange(x, desc(Freq)))
lapply(temp, function(x) head(x,10))    
```







## Number of authors
```{r comment=NA, message=FALSE, warning=FALSE}
n.authors.article <- pi.df %>% 
    filter(attributes == "AU") %>%
    select(id = articleID, author= record) %>%
    mutate(id = as.numeric(id))

n_authors <- n.authors.article %>% 
        group_by(id) %>%
        summarise(n = n())

ggplot(n_authors, aes(x = factor(n))) + 
    geom_bar() + 
    stat_bin(binwidth=1) +
    xlab("Number of Listed Authors") + 
    ylab("Number of Articles")
    

summary(n_authors$n)
```

## Number of authors over time

This figure shows the average number of authors, along with the standard deviation as the ribbon around the average.  Note that there is a possible problem in these data, with a single article listing a huge number.  That can be corrected at a later time. 


```{r message=FALSE, comment=NA, warning=FALSE, fig.height=2, fig.height=4}
df.2 <- tbl_df(pi.df)
year <- df.2 %>%
        filter(attributes == "YR") %>%
        select(id = articleID, year = record)

authors <- df.2 %>%
        filter(attributes == "AU") %>%
        select(id = articleID, author = record)

n_authors <- authors %>%
        group_by(id) %>%
        summarise(n=n())

n_authors <- n_authors %>% 
        left_join(year) %>%
        group_by(year) %>%
        summarise(median.n = median(n),
                  average.n = mean(n),
                  min.n = min(n),
                  max.n = max(n),
                  std.dev  = sd(n) )

plot.author.count2 <- ggplot(n_authors, aes(as.numeric(year), y=average.n, group=1)) + 
    geom_line(colour="black") + 
    geom_ribbon(aes(ymin = average.n-std.dev, ymax=average.n+std.dev), alpha=.2)

head(n_authors, 20)
tail(n_authors, 20)
plot.author.count2
```

# How Many International Contributors?

This section shows a proof of concept -- that is, we can potentially extract all the countries from the author affiliation `AF` tag in the data set.  This involves using a set of regular expressions for the extraction.  Here I have hard-coded a few countries, but I can obtain a file of all countries and use that to automate the process.  We will need to look at the raw data to ensure that the author affiliations have remained in a consistent format throughtout the entirety of the study.  


```{r}
df.affiliations <- pi.df %>%
                filter(attributes == "AF")

us.aff <- ifelse(grepl("US", df.affiliations$record, perl=TRUE) == TRUE, "US", 
          ifelse(grepl("Canada", df.affiliations$record, perl=TRUE ) == TRUE, "Canada", 
          ifelse(grepl("Kong", df.affiliations$record, perl=TRUE ) == TRUE, "Hong Kong", 
          ifelse(grepl("China", df.affiliations$record, perl=TRUE) == TRUE, "China", 
          ifelse(grepl("Israel", df.affiliations$record, perl=TRUE) == TRUE, "Israel", "Other" )))))

affiliations <- data.frame(cbind(df.affiliations,us.aff))

ggplot(data=df.affiliations, aes(x = factor(us.aff))) + geom_bar()
```